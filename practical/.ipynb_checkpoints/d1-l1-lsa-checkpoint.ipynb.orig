{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA\n",
    "\n",
    "In this work you will calculate word embeddings using LSA for arbitrary text and explore their properties.\n",
    "\n",
    "Code from [Yuri Guts's Thrones2Vec](https://github.com/YuriyGuts/thrones2vec/blob/master/Thrones2Vec.ipynb) is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LC_ALL=en_US.UTF-8\n",
      "env: LANG=en_US.UTF-8\n"
     ]
    }
   ],
   "source": [
    "%env LC_ALL=en_US.UTF-8\n",
    "%env LANG=en_US.UTF-8\n",
    "\n",
    "import nltk\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download your text into folder `./data/lab1` in `txt` format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found books: \n",
      "../data/lab1/001ssb.txt\n",
      "../data/lab1/002ssb.txt\n",
      "../data/lab1/003ssb.txt\n",
      "../data/lab1/004ssb.txt\n",
      "../data/lab1/005ssb.txt\n"
     ]
    }
   ],
   "source": [
    "book_filenames = glob.glob(\"../data/lab1/*.txt\")\n",
    "print(\"Found books: \\n{}\".format('\\n'.join(book_filenames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading '../data/lab1/001ssb.txt'...\n",
      "Corpus is now 1607894 characters long.\n",
      "\n",
      "Reading '../data/lab1/002ssb.txt'...\n",
      "Corpus is now 3392960 characters long.\n",
      "\n",
      "Reading '../data/lab1/003ssb.txt'...\n",
      "Corpus is now 5714229 characters long.\n",
      "\n",
      "Reading '../data/lab1/004ssb.txt'...\n",
      "Corpus is now 7437782 characters long.\n",
      "\n",
      "Reading '../data/lab1/005ssb.txt'...\n",
      "Corpus is now 9749792 characters long.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus  = \"\"\n",
    "for book_filename in book_filenames:\n",
    "    print(\"Reading '{0}'...\".format(book_filename))\n",
    "    with open(book_filename, \"r\", ) as f:\n",
    "        corpus += f.read()\n",
    "    print(\"Corpus is now {0} characters long.\\n\".format(len(corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to split our corpus on documents and split document on terms. Use English tokenizers from NLTK or create your own function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/anatolii.stehnii/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# replace document_tokenize and word_tokenize if needed\n",
    "nltk.download(\"punkt\")\n",
    "nltk_english = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "document_tokenize = lambda corpus: nltk_english.tokenize(corpus)\n",
    "\n",
    "from nltk import word_tokenize, sent_tokenize as document_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents_raw = document_tokenize(corpus)\n",
    "documents = list(map(word_tokenize, documents_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your preprocessing if needed or use default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "metadata": {
    "collapsed": true
   },
=======
   "metadata": {},
>>>>>>> testing
   "outputs": [],
   "source": [
    "# replace preprocess if needed\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preprocess(word):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\"\", word)\n",
    "    clean_lower = clean.lower()\n",
    "    return ps.stem(clean_lower)\n",
    "\n",
    "documents = [[preprocess(word) for word in document] for document in documents]\n",
    "documents = [[word for word in document if word] for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents_raw[5])\n",
    "print(documents[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count = sum([len(document) for document in documents])\n",
    "print(\"The corpus contains {0:,} tokens in {1:,} documents\".format(token_count, len(documents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your threshold for a minimum word occurence in the text to filter rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum word count threshold.\n",
    "min_word_count = 3\n",
    "\n",
    "vocab = {}\n",
    "for d in documents:\n",
    "    for w in d:\n",
    "        if w in vocab:\n",
    "            vocab[w] += 1\n",
    "        else:\n",
    "            vocab[w] = 1\n",
    "\n",
    "rare_words = {w for w, count in vocab.items() if count < min_word_count}\n",
    "print(\"The corpus contains {0:,} unique words, {1:,} with less than {2:} occurences.\".format(len(vocab), len(rare_words), min_word_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your own set of stop words or use default from NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace stop_words if needed\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_words = rare_words | stop_words\n",
    "documents = [[word for word in document if word not in remove_words] for document in documents]\n",
    "\n",
    "print(documents[5])\n",
    "all_words = {word for document in documents for word in document}\n",
    "print(\"A filtered corpus contains {} unique words\".format(len(all_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term-document matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you need to transform your corpus into term-document matrix of any type you want (BoW, TF-IDF). Use a sparse matrix, if you have large amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "documents_raw_join = [' '.join(words) for words in documents]\n",
    "td_matrix = vectorizer.fit_transform(documents_raw_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
<<<<<<< HEAD
   "source": [
    "td_matrix = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
=======
>>>>>>> testing
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
