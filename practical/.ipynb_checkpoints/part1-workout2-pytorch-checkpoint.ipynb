{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "Â© Anatolii Stehnii, 2018\n",
    "\n",
    "This assignment should give you a basic understanding of main torch concepts: tensors, broadcasting, ~~variables~~ (obsolete for Pytorch 0.4.0),  autogradient, optimizers, modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor operations\n",
    "For **tensor**, torch has mostly similar to numpy API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0726,  0.7453],\n",
       "        [ 0.7320,  0.9103]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2, 2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2177,  2.2360],\n",
       "        [ 2.1960,  2.7308]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0726,  1.7453],\n",
       "        [ 1.7320,  1.9103]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Element-wise operations with two tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6491,  0.2243],\n",
       "        [ 0.3772,  0.3437]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(2, 2)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7217,  0.9697],\n",
       "        [ 1.1092,  1.2540]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1118,  3.3224],\n",
       "        [ 1.9406,  2.6487]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x / y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You only can perform element-wise operations on tensors with the same number of dimensions; however, sizes can be different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8274,  0.0905]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.rand(1, 2)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0600,  0.0674],\n",
       "        [ 0.6057,  0.0823]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2742],\n",
       "        [ 0.9058]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.rand(2, 1)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2647,  2.7183],\n",
       "        [ 0.8081,  1.0049]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x / w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called **broadcasting**: if we have two tensors of size $m\\times n\\times k$ and $m \\times 1 \\times k$, we can perform element-wise operations with them. The smaller tensor will be virtually clonned $n$ times along the second dimension to fit the size of the larger tensor. https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor **dot product** is possible for two tensors of size $n_1\\times\\ldots n_{i-1}\\times n_i$ and $m_1\\times m_2 \\times\\ldots\\times m_k$ if $n_i = m_1$. The result will have dimensionality $n_1\\times\\ldots\\times n_{i-1}\\times m_2\\times\\ldots\\times m_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3088]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inner product (1 x 2)*(2 x 1) = (1 x 1)\n",
    "z.mm(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2269,  0.0248],\n",
       "        [ 0.7495,  0.0819]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outer product (2 x 1)*(1 x 2) = (2 x 2)\n",
    "w.mm(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  24,   27,   31],\n",
       "        [  43,   47,   56],\n",
       "        [  77,   85,  100],\n",
       "        [  52,   59,   67]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (4 x 2)*(2 x 3) = (4 x 3)\n",
    "w1 = torch.tensor([[1,3], [5,4], [7,8], [1,7]])\n",
    "w2 = torch.tensor([[3,3,4], [7,8,9]])\n",
    "w1.mm(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-bb12c27fb376>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mw1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mw2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mw1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "w1 = torch.rand(2,3,3)\n",
    "w2 = torch.rand(3,2, 2)\n",
    "w1.matmul(w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment\n",
    "Write a code for a forward propagation of a simple neural network with one hidden layer. $\\textbf{x} \\in \\mathbb{R}^{10}, y \\in \\mathbb{R}^{1}, \\textbf{h} \\in \\mathbb{R}^{20}$. Select any nonlinearity you prefer. \n",
    "\n",
    "(Additional) Write down a code for a batch forward propagation of the same network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.random.rand(50, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.rand(10, 20, requires_grad=True)\n",
    "W2 = torch.rand(20, 1, requires_grad=True)\n",
    "X = torch.tensor(X).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MmBackward at 0x11e2eacf8>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = F.relu(X.mm(W1)).mm(W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Autogradient\n",
    "\n",
    "Concept of computational graph is essential part of deep learning, because it allows us not to bother with deriving gradients for backpropagation. Torch `tensor` stores not only numerical data, but it also contains a reference to it's origin: an operation, which created this tensor and related tensors.\n",
    "\n",
    "*Note: In Torch, you are not forced to define a static computational graph before optimization. Order of your computations can be easily changed during execution as easy as for any ordinary algorithm. This way Torch allows to process dynamic structures like trees and graphs.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2696,  0.8314,  0.7390,  0.8474,  0.7649,  0.6143,  0.7942,\n",
       "          0.7798,  0.6302,  0.1675]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "x = torch.rand(1, 5)\n",
    "W = torch.rand(5, 10, requires_grad=True)\n",
    "h = F.tanh(x.mm(W))\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TanhBackward at 0x1171567f0>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<MmBackward at 0x1171569b0>, 0),)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows to calculate an **autogradient** from each scalar, which is a result of operations with other tensors, which require gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0423,  0.0398,  0.0973,  0.0291,  0.0697,  0.0100,  0.0401,\n",
       "          0.0630,  0.0338,  0.0322],\n",
       "        [ 0.3134,  0.2952,  0.7215,  0.2157,  0.5168,  0.0740,  0.2977,\n",
       "          0.4668,  0.2504,  0.2384],\n",
       "        [ 0.1151,  0.1085,  0.2651,  0.0793,  0.1899,  0.0272,  0.1094,\n",
       "          0.1715,  0.0920,  0.0876],\n",
       "        [ 0.0165,  0.0155,  0.0379,  0.0113,  0.0271,  0.0039,  0.0156,\n",
       "          0.0245,  0.0132,  0.0125],\n",
       "        [ 0.2331,  0.2196,  0.5367,  0.1604,  0.3844,  0.0551,  0.2214,\n",
       "          0.3472,  0.1863,  0.1773]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# backward can only be called for scalar\n",
    "if W.grad is not None:\n",
    "    W.grad.data.zero_()\n",
    "h.sum().backward(retain_graph=True)\n",
    "W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment\n",
    "Use provided $\\textbf{X}$ and $\\textbf{y}$ values to find gradient of RMSE loss with respect to parameters of your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = X[:, 0]*X[:, 1] + \\\n",
    "    torch.tensor(np.log(X[:, 2])).float() + \\\n",
    "    (torch.tensor(np.random.rand(5)).float() * X[:, 3:8]).sum(dim=1) + \\\n",
    "    torch.tensor(np.random.normal(0, 0.1, 50)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.965482771396637"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rse_loss(y, y_true):\n",
    "    return torch.sqrt(torch.sum((y - y_true) ** 2))\n",
    "\n",
    "y = F.relu(X.mm(W1)).mm(W2)\n",
    "loss = rmse_loss(y, y_true)/50\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MmBackward at 0x11e2bd5c0>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = W1 - alpha*W1.grad\n",
    "W2 = W2 - alpha*W2.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Optimizer\n",
    "Having a gradient is enough to start training your network with a simple gradient descent; however, you can use more sophisticated optimization methods like Adam or Adagrad. This methods already implemented in torch and available in module `torch.optim` (https://pytorch.org/docs/stable/optim.html#module-torch.optim) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "# any optimizers accepts array of model parameters as the argument; \n",
    "# optimizer specific parameters comes next\n",
    "sgd = SGD([W], lr=0.1)\n",
    "adam = Adam([W], lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_true = torch.tensor([1.0]*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient sum: 0.2197\n",
      "Loss: 0.2295\n"
     ]
    }
   ],
   "source": [
    "sgd.zero_grad()\n",
    "\n",
    "h = F.tanh(x.mm(W))\n",
    "loss = torch.sqrt((h_true**2-h**2).sum())/10\n",
    "\n",
    "loss.backward()\n",
    "print('Gradient sum: {0:.4f}\\nLoss: {1:.4f}'.format(W.grad.abs().sum().item(), loss.item()))\n",
    "sgd.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment\n",
    "Train your simple network with SGD optimizer, learning rate `0.01` and momentum `0.9`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient sum: 45.6538\n",
      "Loss: 20.7527\n",
      "Gradient sum: 43.3473\n",
      "Loss: 19.4472\n",
      "Gradient sum: 39.7143\n",
      "Loss: 17.0063\n",
      "Gradient sum: 36.0490\n",
      "Loss: 13.6116\n",
      "Gradient sum: 32.5564\n",
      "Loss: 9.4453\n",
      "Gradient sum: 29.1474\n",
      "Loss: 4.7003\n",
      "Gradient sum: 16.3909\n",
      "Loss: 1.2550\n",
      "Gradient sum: 30.0169\n",
      "Loss: 4.8444\n",
      "Gradient sum: 31.5699\n",
      "Loss: 7.3291\n",
      "Gradient sum: 32.3008\n",
      "Loss: 8.5187\n",
      "Gradient sum: 32.0718\n",
      "Loss: 8.5748\n",
      "Gradient sum: 30.9307\n",
      "Loss: 7.6639\n",
      "Gradient sum: 29.1122\n",
      "Loss: 5.9482\n",
      "Gradient sum: 26.6150\n",
      "Loss: 3.6000\n",
      "Gradient sum: 11.7205\n",
      "Loss: 1.1451\n",
      "Gradient sum: 23.2191\n",
      "Loss: 2.6393\n",
      "Gradient sum: 25.0427\n",
      "Loss: 4.3725\n",
      "Gradient sum: 25.3926\n",
      "Loss: 5.1605\n",
      "Gradient sum: 25.0413\n",
      "Loss: 5.0949\n",
      "Gradient sum: 23.9928\n",
      "Loss: 4.3047\n",
      "Gradient sum: 22.1355\n",
      "Loss: 2.9359\n",
      "Gradient sum: 14.2979\n",
      "Loss: 1.3105\n",
      "Gradient sum: 18.3267\n",
      "Loss: 1.6309\n",
      "Gradient sum: 22.0279\n",
      "Loss: 2.7988\n",
      "Gradient sum: 22.4641\n",
      "Loss: 3.3183\n",
      "Gradient sum: 22.0168\n",
      "Loss: 3.1936\n",
      "Gradient sum: 20.6669\n",
      "Loss: 2.5274\n",
      "Gradient sum: 16.2149\n",
      "Loss: 1.5110\n",
      "Gradient sum: 6.7659\n",
      "Loss: 1.0575\n",
      "Gradient sum: 16.7450\n",
      "Loss: 1.7771\n",
      "Gradient sum: 18.0929\n",
      "Loss: 2.2178\n",
      "Gradient sum: 17.7529\n",
      "Loss: 2.1701\n",
      "Gradient sum: 15.8628\n",
      "Loss: 1.7094\n",
      "Gradient sum: 8.2222\n",
      "Loss: 1.0995\n",
      "Gradient sum: 9.5504\n",
      "Loss: 1.1345\n",
      "Gradient sum: 15.1809\n",
      "Loss: 1.5692\n",
      "Gradient sum: 15.9103\n",
      "Loss: 1.7203\n",
      "Gradient sum: 14.5662\n",
      "Loss: 1.5263\n",
      "Gradient sum: 9.3211\n",
      "Loss: 1.1380\n",
      "Gradient sum: 3.5054\n",
      "Loss: 1.0039\n",
      "Gradient sum: 11.0988\n",
      "Loss: 1.2616\n",
      "Gradient sum: 12.6921\n",
      "Loss: 1.4134\n",
      "Gradient sum: 11.6249\n",
      "Loss: 1.3167\n",
      "Gradient sum: 7.0352\n",
      "Loss: 1.0763\n",
      "Gradient sum: 2.3472\n",
      "Loss: 0.9887\n",
      "Gradient sum: 8.8870\n",
      "Loss: 1.1405\n",
      "Gradient sum: 10.5679\n",
      "Loss: 1.2371\n",
      "Gradient sum: 9.3149\n",
      "Loss: 1.1672\n",
      "Gradient sum: 4.7487\n",
      "Loss: 1.0191\n",
      "Gradient sum: 2.4023\n",
      "Loss: 0.9883\n",
      "Gradient sum: 7.0704\n",
      "Loss: 1.0849\n",
      "Gradient sum: 8.1975\n",
      "Loss: 1.1320\n",
      "Gradient sum: 6.6385\n",
      "Loss: 1.0722\n",
      "Gradient sum: 2.3970\n",
      "Loss: 0.9871\n",
      "Gradient sum: 2.8657\n",
      "Loss: 0.9909\n",
      "Gradient sum: 5.9978\n",
      "Loss: 1.0514\n",
      "Gradient sum: 6.4361\n",
      "Loss: 1.0655\n",
      "Gradient sum: 4.5536\n",
      "Loss: 1.0174\n",
      "Gradient sum: 0.8307\n",
      "Loss: 0.9751\n",
      "Gradient sum: 2.9221\n",
      "Loss: 0.9919\n",
      "Gradient sum: 4.8530\n",
      "Loss: 1.0263\n",
      "Gradient sum: 4.6958\n",
      "Loss: 1.0230\n",
      "Gradient sum: 2.7213\n",
      "Loss: 0.9891\n",
      "Gradient sum: 0.3427\n",
      "Loss: 0.9727\n",
      "Gradient sum: 2.8799\n",
      "Loss: 0.9903\n",
      "Gradient sum: 3.9329\n",
      "Loss: 1.0068\n",
      "Gradient sum: 3.3590\n",
      "Loss: 0.9971\n",
      "Gradient sum: 1.4857\n",
      "Loss: 0.9765\n",
      "Gradient sum: 0.8547\n",
      "Loss: 0.9733\n",
      "Gradient sum: 2.5487\n",
      "Loss: 0.9865\n",
      "Gradient sum: 2.9947\n",
      "Loss: 0.9923\n",
      "Gradient sum: 2.1944\n",
      "Loss: 0.9824\n",
      "Gradient sum: 0.5474\n",
      "Loss: 0.9717\n",
      "Gradient sum: 1.1756\n",
      "Loss: 0.9739\n",
      "Gradient sum: 2.2198\n",
      "Loss: 0.9820\n",
      "Gradient sum: 2.2599\n",
      "Loss: 0.9824\n",
      "Gradient sum: 1.3895\n",
      "Loss: 0.9749\n",
      "Gradient sum: 0.1836\n",
      "Loss: 0.9703\n",
      "Gradient sum: 1.1853\n",
      "Loss: 0.9737\n",
      "Gradient sum: 1.7718\n",
      "Loss: 0.9778\n",
      "Gradient sum: 1.5680\n",
      "Loss: 0.9761\n",
      "Gradient sum: 0.7345\n",
      "Loss: 0.9713\n",
      "Gradient sum: 0.3281\n",
      "Loss: 0.9700\n",
      "Gradient sum: 1.1503\n",
      "Loss: 0.9728\n",
      "Gradient sum: 1.4197\n",
      "Loss: 0.9744\n",
      "Gradient sum: 1.0890\n",
      "Loss: 0.9723\n",
      "Gradient sum: 0.3507\n",
      "Loss: 0.9697\n",
      "Gradient sum: 0.4493\n",
      "Loss: 0.9699\n",
      "Gradient sum: 0.9712\n",
      "Loss: 0.9717\n",
      "Gradient sum: 1.0339\n",
      "Loss: 0.9720\n",
      "Gradient sum: 0.6607\n",
      "Loss: 0.9703\n",
      "Gradient sum: 0.1568\n",
      "Loss: 0.9691\n",
      "Gradient sum: 0.5313\n",
      "Loss: 0.9697\n",
      "Gradient sum: 0.8360\n",
      "Loss: 0.9706\n",
      "Gradient sum: 0.7723\n",
      "Loss: 0.9703\n",
      "Gradient sum: 0.4038\n",
      "Loss: 0.9692\n",
      "Gradient sum: 0.1697\n",
      "Loss: 0.9687\n",
      "Gradient sum: 0.4857\n",
      "Loss: 0.9693\n",
      "Gradient sum: 0.6376\n",
      "Loss: 0.9697\n",
      "Gradient sum: 0.5071\n",
      "Loss: 0.9692\n",
      "Gradient sum: 0.2164\n",
      "Loss: 0.9686\n",
      "Gradient sum: 0.2017\n",
      "Loss: 0.9685\n",
      "Gradient sum: 0.4585\n",
      "Loss: 0.9689\n",
      "Gradient sum: 0.5082\n",
      "Loss: 0.9690\n",
      "Gradient sum: 0.3489\n",
      "Loss: 0.9686\n",
      "Gradient sum: 0.1180\n",
      "Loss: 0.9682\n",
      "Gradient sum: 0.2291\n",
      "Loss: 0.9683\n",
      "Gradient sum: 0.3653\n",
      "Loss: 0.9685\n",
      "Gradient sum: 0.3517\n",
      "Loss: 0.9684\n",
      "Gradient sum: 0.2152\n",
      "Loss: 0.9681\n",
      "Gradient sum: 0.1081\n",
      "Loss: 0.9680\n",
      "Gradient sum: 0.2299\n",
      "Loss: 0.9681\n",
      "Gradient sum: 0.3131\n",
      "Loss: 0.9682\n",
      "Gradient sum: 0.2633\n",
      "Loss: 0.9680\n",
      "Gradient sum: 0.1265\n",
      "Loss: 0.9679\n",
      "Gradient sum: 0.1350\n",
      "Loss: 0.9678\n",
      "Gradient sum: 0.2091\n",
      "Loss: 0.9679\n",
      "Gradient sum: 0.2342\n",
      "Loss: 0.9679\n",
      "Gradient sum: 0.1885\n",
      "Loss: 0.9678\n",
      "Gradient sum: 0.1132\n",
      "Loss: 0.9677\n",
      "Gradient sum: 0.1132\n",
      "Loss: 0.9676\n",
      "Gradient sum: 0.1807\n",
      "Loss: 0.9677\n",
      "Gradient sum: 0.1819\n",
      "Loss: 0.9676\n",
      "Gradient sum: 0.1197\n",
      "Loss: 0.9675\n",
      "Gradient sum: 0.0945\n",
      "Loss: 0.9675\n",
      "Gradient sum: 0.1418\n",
      "Loss: 0.9675\n",
      "Gradient sum: 0.1672\n",
      "Loss: 0.9675\n",
      "Gradient sum: 0.1564\n",
      "Loss: 0.9674\n",
      "Gradient sum: 0.1172\n",
      "Loss: 0.9674\n",
      "Gradient sum: 0.0846\n",
      "Loss: 0.9673\n",
      "Gradient sum: 0.1053\n",
      "Loss: 0.9673\n",
      "Gradient sum: 0.1220\n",
      "Loss: 0.9673\n",
      "Gradient sum: 0.0993\n",
      "Loss: 0.9673\n",
      "Gradient sum: 0.0807\n",
      "Loss: 0.9672\n",
      "Gradient sum: 0.1023\n",
      "Loss: 0.9672\n",
      "Gradient sum: 0.1255\n",
      "Loss: 0.9672\n",
      "Gradient sum: 0.1274\n",
      "Loss: 0.9672\n",
      "Gradient sum: 0.1093\n",
      "Loss: 0.9671\n",
      "Gradient sum: 0.0840\n",
      "Loss: 0.9671\n",
      "Gradient sum: 0.0761\n",
      "Loss: 0.9671\n",
      "Gradient sum: 0.0829\n",
      "Loss: 0.9671\n",
      "Gradient sum: 0.0799\n",
      "Loss: 0.9671\n",
      "Gradient sum: 0.0731\n",
      "Loss: 0.9670\n",
      "Gradient sum: 0.0821\n",
      "Loss: 0.9670\n",
      "Gradient sum: 0.0980\n",
      "Loss: 0.9670\n",
      "Gradient sum: 0.1044\n",
      "Loss: 0.9670\n",
      "Gradient sum: 0.0977\n",
      "Loss: 0.9670\n",
      "Gradient sum: 0.0819\n",
      "Loss: 0.9669\n",
      "Gradient sum: 0.0700\n",
      "Loss: 0.9669\n",
      "Gradient sum: 0.0687\n",
      "Loss: 0.9669\n",
      "Gradient sum: 0.0691\n",
      "Loss: 0.9669\n",
      "Gradient sum: 0.0662\n",
      "Loss: 0.9669\n",
      "Gradient sum: 0.0715\n",
      "Loss: 0.9668\n",
      "Gradient sum: 0.0803\n",
      "Loss: 0.9668\n",
      "Gradient sum: 0.0872\n",
      "Loss: 0.9668\n",
      "Gradient sum: 0.0861\n",
      "Loss: 0.9668\n",
      "Gradient sum: 0.0780\n",
      "Loss: 0.9668\n",
      "Gradient sum: 0.0694\n",
      "Loss: 0.9668\n",
      "Gradient sum: 0.0630\n",
      "Loss: 0.9667\n",
      "Gradient sum: 0.0610\n",
      "Loss: 0.9667\n",
      "Gradient sum: 0.0609\n",
      "Loss: 0.9667\n",
      "Gradient sum: 0.0648\n",
      "Loss: 0.9667\n",
      "Gradient sum: 0.0696\n",
      "Loss: 0.9667\n",
      "Gradient sum: 0.0749\n",
      "Loss: 0.9667\n",
      "Gradient sum: 0.0760\n",
      "Loss: 0.9667\n",
      "Gradient sum: 0.0725\n",
      "Loss: 0.9666\n",
      "Gradient sum: 0.0666\n",
      "Loss: 0.9666\n",
      "Gradient sum: 0.0620\n",
      "Loss: 0.9666\n",
      "Gradient sum: 0.0591\n",
      "Loss: 0.9666\n",
      "Gradient sum: 0.0585\n",
      "Loss: 0.9666\n",
      "Gradient sum: 0.0601\n",
      "Loss: 0.9666\n",
      "Gradient sum: 0.0629\n",
      "Loss: 0.9666\n",
      "Gradient sum: 0.0661\n",
      "Loss: 0.9665\n",
      "Gradient sum: 0.0679\n",
      "Loss: 0.9665\n",
      "Gradient sum: 0.0667\n",
      "Loss: 0.9665\n",
      "Gradient sum: 0.0631\n",
      "Loss: 0.9665\n",
      "Gradient sum: 0.0597\n",
      "Loss: 0.9665\n",
      "Gradient sum: 0.0573\n",
      "Loss: 0.9665\n",
      "Gradient sum: 0.0562\n",
      "Loss: 0.9665\n",
      "Gradient sum: 0.0565\n",
      "Loss: 0.9665\n",
      "Gradient sum: 0.0579\n",
      "Loss: 0.9664\n",
      "Gradient sum: 0.0599\n",
      "Loss: 0.9664\n",
      "Gradient sum: 0.0615\n",
      "Loss: 0.9664\n",
      "Gradient sum: 0.0613\n",
      "Loss: 0.9664\n",
      "Gradient sum: 0.0596\n",
      "Loss: 0.9664\n",
      "Gradient sum: 0.0569\n",
      "Loss: 0.9664\n",
      "Gradient sum: 0.0548\n",
      "Loss: 0.9664\n",
      "Gradient sum: 0.0537\n",
      "Loss: 0.9664\n",
      "Gradient sum: 0.0534\n",
      "Loss: 0.9664\n",
      "Gradient sum: 0.0539\n",
      "Loss: 0.9664\n",
      "Gradient sum: 0.0553\n",
      "Loss: 0.9663\n",
      "Gradient sum: 0.0564\n",
      "Loss: 0.9663\n",
      "Gradient sum: 0.0567\n",
      "Loss: 0.9663\n",
      "Gradient sum: 0.0559\n",
      "Loss: 0.9663\n",
      "Gradient sum: 0.0543\n",
      "Loss: 0.9663\n",
      "Gradient sum: 0.0525\n",
      "Loss: 0.9663\n",
      "Gradient sum: 0.0512\n",
      "Loss: 0.9663\n",
      "Gradient sum: 0.0506\n",
      "Loss: 0.9663\n",
      "Gradient sum: 0.0509\n",
      "Loss: 0.9663\n",
      "Gradient sum: 0.0517\n",
      "Loss: 0.9663\n",
      "Gradient sum: 0.0524\n",
      "Loss: 0.9663\n",
      "Gradient sum: 0.0527\n",
      "Loss: 0.9662\n",
      "Gradient sum: 0.0524\n",
      "Loss: 0.9662\n",
      "Gradient sum: 0.0514\n",
      "Loss: 0.9662\n",
      "Gradient sum: 0.0503\n",
      "Loss: 0.9662\n",
      "Gradient sum: 0.0492\n",
      "Loss: 0.9662\n",
      "Gradient sum: 0.0486\n",
      "Loss: 0.9662\n",
      "Gradient sum: 0.0484\n",
      "Loss: 0.9662\n",
      "Gradient sum: 0.0487\n",
      "Loss: 0.9662\n",
      "Gradient sum: 0.0491\n",
      "Loss: 0.9662\n",
      "Gradient sum: 0.0493\n",
      "Loss: 0.9662\n",
      "Gradient sum: 0.0492\n",
      "Loss: 0.9662\n",
      "Gradient sum: 0.0487\n",
      "Loss: 0.9662\n",
      "Gradient sum: 0.0479\n",
      "Loss: 0.9662\n",
      "Gradient sum: 0.0471\n",
      "Loss: 0.9661\n",
      "Gradient sum: 0.0465\n",
      "Loss: 0.9661\n",
      "Gradient sum: 0.0462\n",
      "Loss: 0.9661\n",
      "Gradient sum: 0.0462\n",
      "Loss: 0.9661\n",
      "Gradient sum: 0.0463\n",
      "Loss: 0.9661\n",
      "Gradient sum: 0.0464\n",
      "Loss: 0.9661\n",
      "Gradient sum: 0.0463\n",
      "Loss: 0.9661\n",
      "Gradient sum: 0.0460\n",
      "Loss: 0.9661\n",
      "Gradient sum: 0.0455\n",
      "Loss: 0.9661\n",
      "Gradient sum: 0.0449\n",
      "Loss: 0.9661\n",
      "Gradient sum: 0.0444\n",
      "Loss: 0.9661\n",
      "Gradient sum: 0.0440\n",
      "Loss: 0.9661\n",
      "Gradient sum: 0.0439\n",
      "Loss: 0.9661\n",
      "Gradient sum: 0.0438\n",
      "Loss: 0.9661\n",
      "Gradient sum: 0.0438\n",
      "Loss: 0.9661\n",
      "Gradient sum: 0.0438\n",
      "Loss: 0.9661\n",
      "Gradient sum: 0.0435\n",
      "Loss: 0.9660\n",
      "Gradient sum: 0.0432\n",
      "Loss: 0.9660\n",
      "Gradient sum: 0.0427\n",
      "Loss: 0.9660\n",
      "Gradient sum: 0.0423\n",
      "Loss: 0.9660\n",
      "Gradient sum: 0.0420\n",
      "Loss: 0.9660\n",
      "Gradient sum: 0.0417\n",
      "Loss: 0.9660\n",
      "Gradient sum: 0.0416\n",
      "Loss: 0.9660\n",
      "Gradient sum: 0.0415\n",
      "Loss: 0.9660\n",
      "Gradient sum: 0.0414\n",
      "Loss: 0.9660\n",
      "Gradient sum: 0.0412\n",
      "Loss: 0.9660\n",
      "Gradient sum: 0.0410\n",
      "Loss: 0.9660\n",
      "Gradient sum: 0.0406\n",
      "Loss: 0.9660\n",
      "Gradient sum: 0.0403\n",
      "Loss: 0.9660\n",
      "Gradient sum: 0.0400\n",
      "Loss: 0.9660\n",
      "Gradient sum: 0.0397\n",
      "Loss: 0.9660\n",
      "Gradient sum: 0.0396\n",
      "Loss: 0.9660\n",
      "Gradient sum: 0.0394\n",
      "Loss: 0.9660\n",
      "Gradient sum: 0.0393\n",
      "Loss: 0.9660\n",
      "Gradient sum: 0.0391\n",
      "Loss: 0.9660\n",
      "Gradient sum: 0.0389\n",
      "Loss: 0.9660\n",
      "Gradient sum: 0.0387\n",
      "Loss: 0.9659\n",
      "Gradient sum: 0.0384\n",
      "Loss: 0.9659\n",
      "Gradient sum: 0.0381\n",
      "Loss: 0.9659\n",
      "Gradient sum: 0.0379\n",
      "Loss: 0.9659\n",
      "Gradient sum: 0.0377\n",
      "Loss: 0.9659\n",
      "Gradient sum: 0.0375\n",
      "Loss: 0.9659\n",
      "Gradient sum: 0.0374\n",
      "Loss: 0.9659\n",
      "Gradient sum: 0.0372\n",
      "Loss: 0.9659\n",
      "Gradient sum: 0.0370\n",
      "Loss: 0.9659\n",
      "Gradient sum: 0.0368\n",
      "Loss: 0.9659\n",
      "Gradient sum: 0.0365\n",
      "Loss: 0.9659\n",
      "Gradient sum: 0.0363\n",
      "Loss: 0.9659\n",
      "Gradient sum: 0.0361\n",
      "Loss: 0.9659\n",
      "Gradient sum: 0.0359\n",
      "Loss: 0.9659\n",
      "Gradient sum: 0.0357\n",
      "Loss: 0.9659\n",
      "Gradient sum: 0.0355\n",
      "Loss: 0.9659\n",
      "Gradient sum: 0.0354\n",
      "Loss: 0.9659\n",
      "Gradient sum: 0.0352\n",
      "Loss: 0.9659\n",
      "Gradient sum: 0.0350\n",
      "Loss: 0.9659\n",
      "Gradient sum: 0.0348\n",
      "Loss: 0.9659\n",
      "Gradient sum: 0.0346\n",
      "Loss: 0.9659\n",
      "Gradient sum: 0.0344\n",
      "Loss: 0.9659\n",
      "Gradient sum: 0.0342\n",
      "Loss: 0.9659\n",
      "Gradient sum: 0.0340\n",
      "Loss: 0.9659\n",
      "Gradient sum: 0.0339\n",
      "Loss: 0.9659\n",
      "Gradient sum: 0.0337\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0335\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0334\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0332\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0330\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0328\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0326\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0324\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0323\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0321\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0320\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0318\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0316\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0315\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0313\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0311\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0309\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0308\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0306\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0305\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0303\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0302\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0300\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0299\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0297\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0296\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0294\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0293\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0291\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0290\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0288\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0287\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0285\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0284\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0282\n",
      "Loss: 0.9658\n",
      "Gradient sum: 0.0281\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0280\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0278\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0277\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0276\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0274\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0273\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0271\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0270\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0269\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0267\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0266\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0265\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0263\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0262\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0261\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0260\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0258\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0257\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0256\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0254\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0253\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0252\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0251\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0250\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0248\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0247\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0246\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0245\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0244\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0242\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0241\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0240\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0239\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0238\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0237\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0235\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0234\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0233\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0232\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0231\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0230\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0229\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0228\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0227\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0226\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0225\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0224\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0223\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0222\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0221\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0220\n",
      "Loss: 0.9657\n",
      "Gradient sum: 0.0219\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0218\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0217\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0216\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0215\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0214\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0213\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0212\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0211\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0210\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0209\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0208\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0207\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0206\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0205\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0204\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0203\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0202\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0201\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0201\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0200\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0199\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0198\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0197\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0196\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0195\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0194\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0193\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0193\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0192\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0191\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0190\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0189\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0188\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0187\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0187\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0186\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0185\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0184\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0183\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0182\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0182\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0181\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0180\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0179\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0178\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0178\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0177\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0176\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0175\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0175\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0174\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0173\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0172\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0171\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0171\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0170\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0169\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0168\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0168\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0167\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0166\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0165\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0165\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0164\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0163\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0163\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0162\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0161\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0160\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0160\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0159\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0158\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0158\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0157\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0156\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0156\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0155\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0154\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0154\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0153\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0153\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0152\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0151\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0151\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0150\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0149\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0149\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0148\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0148\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0147\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0146\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0146\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0145\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0145\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0144\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0143\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0143\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0142\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0142\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0141\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0140\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0140\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0139\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0139\n",
      "Loss: 0.9656\n",
      "Gradient sum: 0.0138\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0138\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0137\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0136\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0136\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0135\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0135\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0134\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0134\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0133\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0133\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0132\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0132\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0131\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0130\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0130\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0129\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0129\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0128\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0128\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0127\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0127\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0126\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0126\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0125\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0125\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0124\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0124\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0123\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0123\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0122\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0122\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0121\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0121\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0120\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0120\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0119\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0119\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0118\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0118\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0117\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0117\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0116\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0116\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0115\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0115\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0114\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0114\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0114\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0113\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0113\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0112\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0112\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0111\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0111\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0110\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0110\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0109\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0109\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0109\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0108\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0108\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0107\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0107\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0106\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0106\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0106\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0105\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0105\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0104\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0104\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0103\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0103\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0103\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0102\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0102\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0101\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0101\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0101\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0100\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0100\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0099\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0099\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0099\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0098\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0098\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0097\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0097\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0097\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0096\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0096\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0095\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0095\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0095\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0094\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0094\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0093\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0093\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0093\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0092\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0092\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0092\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0091\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0091\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0091\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0090\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0090\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0089\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0089\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0089\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0088\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0088\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0088\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0087\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0087\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0087\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0086\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0086\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0086\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0085\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0085\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0085\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0084\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0084\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0084\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0083\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0083\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0083\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0082\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0082\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0082\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0081\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0081\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0081\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0080\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0080\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0080\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0079\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0079\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0079\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0078\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0078\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0078\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0077\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0077\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0077\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0076\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0076\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0076\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0076\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0075\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0075\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0075\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0074\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0074\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0074\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0073\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0073\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0073\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0073\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0072\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0072\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0072\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0071\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0071\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0071\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0071\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0070\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0070\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0070\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0069\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0069\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0069\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0069\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0068\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0068\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0068\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0068\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0067\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0067\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0067\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0067\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0066\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0066\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0066\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0065\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0065\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0065\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0065\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0064\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0064\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0064\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0064\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0063\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0063\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0063\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0063\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0062\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0062\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0062\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0062\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0061\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0061\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0061\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0061\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0060\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0060\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0060\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0060\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0060\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0059\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0059\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0059\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0059\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0058\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0058\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0058\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0058\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0057\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0057\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0057\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0057\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0057\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0056\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0056\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.9655\n",
      "Gradient sum: 0.0056\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0055\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0055\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0055\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0055\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0055\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0054\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0054\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0054\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0054\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0054\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0053\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0053\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0053\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0053\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0052\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0052\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0052\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0052\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0052\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0051\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0051\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0051\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0051\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0051\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0050\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0050\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0050\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0050\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0050\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0049\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0049\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0049\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0049\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0049\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0049\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0048\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0048\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0048\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0048\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0048\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0047\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0047\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0047\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0047\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0047\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0046\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0046\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0046\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0046\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0046\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0046\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0045\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0045\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0045\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0045\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0045\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0045\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0044\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0044\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0044\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0044\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0044\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0044\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0043\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0043\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0043\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0043\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0043\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0043\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0042\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0042\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0042\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0042\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0042\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0042\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0041\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0041\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0041\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0041\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0041\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0041\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0040\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0040\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0040\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0040\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0040\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0040\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0039\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0039\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0039\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0039\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0039\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0039\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0039\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0038\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0038\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0038\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0038\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0038\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0038\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0038\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0037\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0037\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0037\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0037\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0037\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0037\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0037\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0036\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0036\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0036\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0036\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0036\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0036\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0036\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0035\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0035\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0035\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0035\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0035\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0035\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0035\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0034\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0034\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0034\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0034\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0034\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0034\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0034\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0034\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0033\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0033\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0033\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0033\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0033\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0033\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0033\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0033\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0032\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0032\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0032\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0032\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0032\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0032\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0032\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0032\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0031\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0031\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0031\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0031\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0031\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0031\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0031\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0031\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0031\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0030\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0030\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0030\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0030\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0030\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0030\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0030\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0030\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0029\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0029\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0029\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0029\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0029\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0029\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0029\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0029\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0029\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0028\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0028\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0028\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0028\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0028\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0028\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0028\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0028\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0028\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0028\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0027\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0027\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0027\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0027\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0027\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0027\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0027\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0027\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0027\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0026\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0026\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0026\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0026\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0026\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0026\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0026\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0026\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0026\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0026\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0025\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0025\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0025\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0025\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0025\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0025\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0025\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0025\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0025\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0025\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0025\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0024\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0024\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0024\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0024\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0024\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0024\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0024\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0024\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0024\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0024\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0024\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0023\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0023\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0023\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0023\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0023\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0023\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0023\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0023\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0023\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0023\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0023\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0023\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0022\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0022\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0022\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0022\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0022\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0022\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0022\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0022\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0022\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0022\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0022\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0022\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0021\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0021\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0021\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0021\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0021\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0021\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0021\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0021\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0021\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0021\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0021\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0021\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0020\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0020\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0020\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0020\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0020\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0020\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0020\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0020\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0020\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0020\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0020\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0020\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0020\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0020\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0019\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0019\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0019\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0019\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0019\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0019\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0019\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0019\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0019\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0019\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0019\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0019\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0019\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0019\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0019\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0018\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0018\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0018\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0018\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0018\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0018\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0018\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0018\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0018\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0018\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0018\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0018\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0018\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0018\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0018\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0018\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0017\n",
      "Loss: 0.9655\n",
      "Gradient sum: 0.0017\n",
      "Loss: 0.9655\n"
     ]
    }
   ],
   "source": [
    "W1 = torch.rand(10, 20, requires_grad=True)\n",
    "W2 = torch.rand(20, 1, requires_grad=True)\n",
    "sgd = SGD([W1, W2], lr=0.01, momentum=0.9)\n",
    "\n",
    "for i in range(1000):\n",
    "    sgd.zero_grad()\n",
    "\n",
    "    y = F.relu(X.mm(W1)).mm(W2)\n",
    "    loss = rse_loss(y, y_true)/50\n",
    "\n",
    "    loss.backward()\n",
    "    print('Gradient sum: {0:.4f}\\nLoss: {1:.4f}'.format(W1.grad.abs().sum().item(), loss.item()))\n",
    "    sgd.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Module\n",
    "\n",
    "At this point, you already have everything you need to start training your network. Module is simply a handy way to organize your code into a separate unit and provide optimizer with a list of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, loss 9.7928, target 1.87\n",
      "Epoch 20, loss 4.9344, target 2.78\n",
      "Epoch 30, loss 2.0424, target 3.57\n",
      "Epoch 40, loss 0.6333, target 4.20\n",
      "Epoch 50, loss 0.1207, target 4.65\n",
      "Epoch 60, loss 0.0061, target 4.92\n",
      "Epoch 70, loss 0.0023, target 5.05\n",
      "Epoch 80, loss 0.0062, target 5.08\n",
      "Epoch 90, loss 0.0039, target 5.06\n",
      "Epoch 100, loss 0.0011, target 5.03\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "class SimpleModule(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(SimpleModule, self).__init__()\n",
    "        self.W = nn.Parameter(torch.empty(in_dim, out_dim))\n",
    "        init.normal_(self.W)\n",
    "        self.b = nn.Parameter(torch.zeros(out_dim))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return F.tanh(X.mm(self.W)) + self.b\n",
    "    \n",
    "model = SimpleModule(10, 1)\n",
    "optimizer = Adam(model.parameters(), lr=0.1)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "x = torch.rand(1, 10)\n",
    "y_true = torch.tensor([[5.0]])\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    y = model(x)\n",
    "    l = loss(y, y_true)\n",
    "    l.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print('Epoch {0}, loss {1:.4f}, target {2:.2f}'.format(epoch+1, l.item(), y.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
