{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA\n",
    "\n",
    "© Anatolii Stehnii, 2018\n",
    "\n",
    "Main goal of this assignment is to give you a basic understanding of how Latent Semantic Analysis is performed and how to interpret it results. In this work you will calculate word embeddings using LSA for arbitrary text and explore their properties.\n",
    "\n",
    "Code from [Yuri Guts's Thrones2Vec](https://github.com/YuriyGuts/thrones2vec/blob/master/Thrones2Vec.ipynb) is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LC_ALL=en_US.UTF-8\n",
      "env: LANG=en_US.UTF-8\n"
     ]
    }
   ],
   "source": [
    "%env LC_ALL=en_US.UTF-8\n",
    "%env LANG=en_US.UTF-8\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download any text and save it in `./data/part1/corpus.txt`. I recommend you to use English text, but if you feel brave enough, you can setup a pipleline for any other language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus is 9748084 characters long.\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data/part1\"\n",
    "corpus_file = os.path.join(data_dir, \"corpus.txt\")\n",
    "\n",
    "with open(corpus_file, \"r\", ) as f:\n",
    "    corpus = f.read()\n",
    "print(\"Corpus is {0} characters long.\".format(len(corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to split our corpus on documents and split documents on terms. Use English tokenizers from NLTK or create your own function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/anatolii.stehnii/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk_english = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# use this or define your own document_tokenize\n",
    "document_tokenize = lambda corpus: nltk_english.tokenize(corpus)\n",
    "\n",
    "# use this or define your own word_tokenize\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split corpus on documents\n",
    "documents_raw = document_tokenize(corpus)\n",
    "# split documents on tokens\n",
    "documents = list(map(word_tokenize, documents_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your preprocessing if needed or use default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# default preprocessing – leave only alphabetical characters and stem tokens\n",
    "# redefine this function, if you need another pipeline\n",
    "def preprocess(word):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\"\", word)\n",
    "    clean_lower = clean.lower()\n",
    "    return ps.stem(clean_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace each word with preprocessed\n",
    "documents = [[preprocess(word) for word in document] for document in documents]\n",
    "# filter empty tokens\n",
    "documents = [[word for word in document if word] for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw document: He was an old man, past fifty, and he had seen the lordlings come and go.\n",
      "\n",
      "Preprocessed document: ['he', 'wa', 'an', 'old', 'man', 'past', 'fifti', 'and', 'he', 'had', 'seen', 'the', 'lordl', 'come', 'and', 'go']\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "The corpus contains 1,807,559 tokens in 158,726 documents\n"
     ]
    }
   ],
   "source": [
    "# Compare raw and preprocessed documents\n",
    "print(\"Raw document: {}\\n\".format(documents_raw[5]))\n",
    "print(\"Preprocessed document: {}\\n\".format(documents[5]))\n",
    "print(\"-\"*80)\n",
    "token_count = sum([len(document) for document in documents])\n",
    "print(\"The corpus contains {0:,} tokens in {1:,} documents\".format(token_count, len(documents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are filtering rare words from our documents. We clearly will not be able to analyze a word, if it occurs only a few times in a text. Define your threshold for a minimum word occurence in the text to filter rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus vocabulary contains 16,232 unique words, 6,080 with less than 3 occurences.\n"
     ]
    }
   ],
   "source": [
    "min_word_count = 3\n",
    "\n",
    "words_count = {}\n",
    "for d in documents:\n",
    "    for w in d:\n",
    "        if w in words_count:\n",
    "            words_count[w] += 1\n",
    "        else:\n",
    "            words_count[w] = 1\n",
    "\n",
    "rare_words = {w for w, count in words_count.items() if count < min_word_count}\n",
    "print(\"The corpus vocabulary contains {0:,} unique words, {1:,} with less than {2:} occurences.\".format(len(words_count), len(rare_words), min_word_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your own set of stop words or use default from NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/anatolii.stehnii/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#stop_words = {'a', 'an', 'the'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered document: ['wa', 'old', 'man', 'past', 'fifti', 'seen', 'lordl', 'come', 'go']\n",
      "--------------------------------------------------------------------------------\n",
      "A filtered vocabulary contains 10030 unique words\n"
     ]
    }
   ],
   "source": [
    "# remove stop words and rare words from documents\n",
    "remove_words = rare_words | stop_words\n",
    "documents = [[word for word in document if word not in remove_words] for document in documents]\n",
    "print(\"Filtered document: {}\" .format(documents[5]))\n",
    "print(\"-\"*80)\n",
    "\n",
    "all_words = {word for document in documents for word in document}\n",
    "vocabulary = {w:i for i, w in enumerate(sorted(all_words))}\n",
    "print(\"A filtered vocabulary contains {} unique words\".format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term-document matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you need to transform your corpus into term-document matrix of any type you want (BoW, TF-IDF). Use a sparse matrix, if you have large amount of data. Don't forget to save your vocabulary so you can restore a row index for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_idf_matrix = None # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD\n",
    "\n",
    "Perform SVD of term-document matrix and reduce it's dimensionality to n_dim components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_dim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u = None # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embeddings = u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a set of words you would like to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = ['king', 'queen', 'robert', 'cersei', 'daeneri', 'stanni', 'man', 'woman']\n",
    "indexes = [vocabulary[word] for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a plot with your words, use first two components for x and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure distances (cosine, euclidean) between words to check, if distances have meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to find meaning for individual components\n",
    "component = 1\n",
    "values = word_ebeddings[:,component].tolist()\n",
    "words_values = list(sorted(zip(vocabulary.items(), values), key=lambda x: x[1]))\n",
    "words_values[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_values[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Save results\n",
    "We will need preprocessed dataset and word embeddings in the next assignment, so let's save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store vocabulary as word per line. Indexes can be easily restored from order, therore they are ommited\n",
    "with open(os.path.join(data_dir, 'vocabulary.txt'), \"w\") as f:\n",
    "    vocab_str = '\\n'.join(vocabulary.keys())\n",
    "    f.write(vocab_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store corpus, document per line, each token replaced with index\n",
    "with open(os.path.join(data_dir, 'corpus_preprocessed.txt'), \"w\") as f:\n",
    "    corpus_str = '\\n'.join([' '.join([str(vocabulary[token]) for token in document]) for document in documents])\n",
    "    f.write(corpus_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store LSA embeddings\n",
    "np.save(os.path.join(data_dir, 'lsa_embeddings.npy'), word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
