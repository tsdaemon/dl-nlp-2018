{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice: Specific Neural Architectures for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Yuriy Guts_**\n",
    "\n",
    "_UCU NLP Summer School, 2018_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on a tutorial by Sean Robertson (<https://github.com/spro/practical-pytorch>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be building and training a basic character-level RNN to classify\n",
    "words. A character-level RNN reads words as a series of characters -\n",
    "outputting a prediction and \"hidden state\" at each step, feeding its\n",
    "previous hidden state into each next step. We take the final prediction\n",
    "to be the output, i.e. which class the word belongs to.\n",
    "\n",
    "Specifically, we'll train on a few thousand surnames from 18 languages\n",
    "of origin, and predict which language a name is from based on the\n",
    "spelling:\n",
    "\n",
    "```\n",
    "> Hinton\n",
    "(-0.47) Scottish\n",
    "(-1.52) English\n",
    "(-3.57) Irish\n",
    "\n",
    "> Schmidhuber\n",
    "(-0.19) German\n",
    "(-2.48) Czech\n",
    "(-2.68) Dutch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "import unicodedata\n",
    "\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Included in the `data/names` directory are 18 text files named like `[Country].txt`. Each file contains a bunch of names, one name per line, mostly romanized (but we still need to convert from Unicode to ASCII).\n",
    "\n",
    "We'll end up with a dictionary of lists of names per country, {country: [names ...]}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_files = glob.glob('../data/part2/names/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_characters = string.ascii_letters + \" .,;'\"\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in unique_characters\n",
    "    )\n",
    "\n",
    "test_name = 'Ślusàrski'\n",
    "print(test_name, '->', unicode_to_ascii(test_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the category_lines dictionary, a list of names per country\n",
    "examples_by_class = {}\n",
    "unique_classes = []\n",
    "\n",
    "def read_country_names(filename):\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        lines = f.read().strip().split('\\n')\n",
    "        return [unicode_to_ascii(line) for line in lines]\n",
    "\n",
    "for filename in data_files:\n",
    "    country = os.path.splitext(os.path.basename(filename))[0]\n",
    "    unique_classes.append(country)\n",
    "    names = read_country_names(filename)\n",
    "    examples_by_class[country] = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have ``examples_by_class``, a dictionary mapping each class\n",
    "(country) to a list of words (names).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(examples_by_class['Polish'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the names organized, we need to turn them into Tensors to make any use of them.\n",
    "\n",
    "To represent a single letter, we use a \"one-hot vector\" of size `<1 x len(unique_characters)>`. A one-hot vector is filled with 0-s except for a 1 at index of the current letter, e.g. `\"b\" = <0 1 0 0 0 ...>.`\n",
    "\n",
    "To make a word we join a bunch of those into a 2D matrix `<word_length x 1 x len(unique_characters)>`.\n",
    "\n",
    "That extra 1 dimension is because PyTorch assumes everything is in batches - we're just using a batch size of 1 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "character_to_index = dict(zip(unique_characters, range(len(unique_characters))))\n",
    "class_to_index = dict(zip(unique_classes, range(len(unique_classes))))\n",
    "index_to_class = {v: k for k, v in class_to_index.items()}\n",
    "\n",
    "def character_to_tensor(char):\n",
    "    # TODO: Your code here.\n",
    "    # Return a torch tensor for the given character.\n",
    "\n",
    "def word_to_tensor(word):\n",
    "    # TODO: Your code here.\n",
    "    # Return a torch tensor for the given word.\n",
    "\n",
    "def class_to_tensor(cls):\n",
    "    # TODO: Your code here.\n",
    "    # Return a torch tensor representing the supervised label for the given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(character_to_tensor('B'))\n",
    "print(character_to_tensor('B').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_to_tensor('Bishop').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        # TODO: Your code here.\n",
    "        # Add two linear layers (one for R and one for O function).\n",
    "        # Add a LogSoftmax layer for the output.\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # TODO: Implement forward prop and return the output vector and the hidden state (2 return values).\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # TODO: Reset the initial state of the RNN (return a tensor with all zeroes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden = 128\n",
    "rnn = RNN(len(unique_characters), n_hidden, len(unique_classes))\n",
    "\n",
    "loss_func = nn.NLLLoss()\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a step of this network we need to pass an input (in our case, the\n",
    "Tensor for the current character) and a previous hidden state (which we\n",
    "initialize as zeros at first). We'll get back the output (probability of\n",
    "each country) and a next hidden state (which we keep for the next\n",
    "step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input = character_to_tensor('B')\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "output, next_hidden = rnn(input, hidden)\n",
    "\n",
    "print(output.size())\n",
    "print(next_hidden.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of efficiency we don't want to be creating a new Tensor for\n",
    "every step, so we will use `word_to_tensor`` instead of\n",
    "``character_to_tensor`` and use slices. This could be further optimized by\n",
    "pre-computing batches of Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = word_to_tensor('Bishop')\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = rnn(input[0], hidden)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the output is a ``<1 x len(unique_classes)>`` Tensor, where\n",
    "every item is the likelihood of that category (higher is more likely)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_random_training_sample():\n",
    "    cls = random.choice(unique_classes)\n",
    "    word = random.choice(examples_by_class[cls])\n",
    "    word_tensor = word_to_tensor(word)\n",
    "    class_tensor = class_to_tensor(cls)\n",
    "    return word, cls, word_tensor, class_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each loop of training will:\n",
    "\n",
    "-  Create input and target tensors\n",
    "-  Create a zeroed initial hidden state\n",
    "-  Read each letter in and keep hidden state for next letter\n",
    "-  Compare final output to target\n",
    "-  Back-propagate\n",
    "-  Return the output and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_iter(word_tensor, class_tensor):\n",
    "    hidden = rnn.init_hidden()\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    for i in range(word_tensor.size()[0]):\n",
    "        output, hidden = rnn(word_tensor[i], hidden)\n",
    "\n",
    "    loss = loss_func(output, class_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_output_to_class_label(output):\n",
    "    # TODO: Your code here.\n",
    "    # Given the output vector of the RNN, return the class string and the class number (2 return values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just have to run that with a bunch of examples. Since the\n",
    "``train`` function returns both the output and loss we can print its\n",
    "guesses and also keep track of loss for plotting. Since there are 1000s\n",
    "of examples we print only every ``print_every`` examples, and take an\n",
    "average of the loss.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 100000\n",
    "print_every = 5000\n",
    "plot_every = 1000\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def time_since(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    word, cls, word_tensor, class_tensor = get_random_training_sample()\n",
    "    output, loss = train_iter(word_tensor, class_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = nn_output_to_class_label(output)\n",
    "        correct = '✓' if guess == cls else '✗ (%s)' % cls\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, time_since(start), loss, word, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnose the Results\n",
    "\n",
    "Plotting the historical loss from ``all_losses`` shows the network\n",
    "learning:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just return an output given a word tensor.\n",
    "def nn_output(word_tensor):\n",
    "    # TODO: Your code here.\n",
    "    # Reset the hidden state of the neural network and return the output vector for the given word tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_top_k_classes(input_word, k=3):\n",
    "    print()\n",
    "    print('Predicting:', input_word)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = nn_output(word_to_tensor(input_word))\n",
    "\n",
    "        # Get top N categories\n",
    "        topv, topi = output.topk(k, 1, True)\n",
    "        predictions = []\n",
    "\n",
    "        for i in range(k):\n",
    "            value = topv[0][i].item()\n",
    "            category_index = topi[0][i].item()\n",
    "            print('(%.2f) %s' % (value, index_to_class[category_index]))\n",
    "            predictions.append([value, index_to_class[category_index]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the Results\n",
    "======================\n",
    "\n",
    "To see how well the network performs on different categories, we will\n",
    "create a confusion matrix, indicating for every actual language (rows)\n",
    "which language the network guesses (columns). To calculate the confusion\n",
    "matrix a bunch of samples are run through the network with\n",
    "``nn_output()``, which is the same as ``train()`` minus the backprop.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(len(unique_classes), len(unique_classes))\n",
    "n_confusion = 10000\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    word, cls, word_tensor, class_tensor = get_random_training_sample()\n",
    "    output = nn_output(word_tensor)\n",
    "    guess, guess_idx = nn_output_to_class_label(output)\n",
    "    class_idx = class_to_index[cls]\n",
    "    confusion[class_idx][guess_idx] += 1\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(len(unique_classes)):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + unique_classes, rotation=90)\n",
    "ax.set_yticklabels([''] + unique_classes)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try your own input!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_top_k_classes('Shevchenko')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
