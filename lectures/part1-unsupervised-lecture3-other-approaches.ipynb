{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Usupervised learning\n",
    "\n",
    "© Anatolii Stehnii, 2018\n",
    "\n",
    "## Lecture 3: Other approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        width:800px;\n",
       "        margin-left:16% !important;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: Helvetica, serif;\n",
       "    }\n",
       "    h4{\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "    div.text_cell_render{\n",
       "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 145%;\n",
       "        font-size: 130%;\n",
       "        width:800px;\n",
       "        margin-left:auto;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "    }\n",
       "    .prompt{\n",
       "        display: None;\n",
       "    }\n",
       "\n",
       "    .text_cell_render h4 {\n",
       "        text-decoration: ;\n",
       "    }\n",
       "\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 22pt;\n",
       "        color: #4057A1;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    \n",
       "    .warning {\n",
       "        color: rgb( 240, 20, 20 )\n",
       "    }\n",
       "\n",
       "    div.warn {\n",
       "        background-color: #fcf2f2;\n",
       "        border-color: #dFb5b4;\n",
       "        border-left: 5px solid #dfb5b4;\n",
       "        padding: 0.5em;\n",
       "    }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"../custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec is a classical example of how deep learning can be used to find word embeddings. However, for practical usage you can consider more novel and sophisticated methods. In this lecture I will give an overview of possible approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim word2vec implementation\n",
    "\n",
    "First of all, you probably shall not invent a bicycle and write your own word2vec for a practical task. Gensim already did this for you.\n",
    "\n",
    "Besides fast and accurate implementation of Word2vec, their model also  contains useful tools, like finding most similar word given positive and negative words:\n",
    "```python\n",
    ">>> model.wv.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "[('queen', 0.50882536), ...]\n",
    "```\n",
    "\n",
    "Practical example of Word2vec trained on custom corpus can be found in Yurii's Guts [Thrones2Vec](https://github.com/YuriyGuts/thrones2vec) project.\n",
    "\n",
    "[Original paper](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "\n",
    "[Another paper](https://arxiv.org/pdf/1310.4546.pdf)\n",
    "\n",
    "[Project page](https://radimrehurek.com/gensim/models/word2vec.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe: global vector representation\n",
    "\n",
    "This methods, developed in Stanford, also uses a neural model to build an unsupervised probabilistic model of corpus; however, GloVe approach is different. Word2vec model minimize a cross-entropy loss of classifier, which predicts if a word will occur in prescence of another word. GloVe first counts a square matrix of word co-occurance log-probabilities and then tries to optimize word embeddings so dot product of two word vectors is equal to a value in this matrix for this two words. \n",
    "\n",
    "> The training objective of GloVe is to learn word vectors such that their dot product equals the logarithm of the words’ probability of co-occurrence.\n",
    "\n",
    "\n",
    "![GloVe vs Word2vec performance](glove-vs-word2vec.png)\n",
    "\n",
    "[Original paper](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "\n",
    "[Project page](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "[Nice article about topic](https://blog.acolyer.org/2016/04/22/glove-global-vectors-for-word-representation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText\n",
    "\n",
    "This method from Facebook research group trains skip-gram model not for exact words, but for character n-grams. Target vector is a Bag-Of-Grams vector of a context word. For example, word `<where>` can be represented by 3-grams:\n",
    "\n",
    "```\n",
    "<wh\n",
    "whe\n",
    "her\n",
    "ere\n",
    "re>\n",
    "```\n",
    "\n",
    "This approach has multiple advantages:\n",
    "\n",
    "1. Input sparsity reduced.\n",
    "2. Model can capture morphological similarity of words and incorporate this information.\n",
    "3. Vectors for unknown words can be approximated.\n",
    "\n",
    "[Original paper](https://arxiv.org/abs/1607.04606)\n",
    "\n",
    "[Project GitHub](https://github.com/facebookresearch/fastText/#introduction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MUSE\n",
    "\n",
    "This project addressed a problem of multilingual words representations. MUSE decribes methods of aligning of vector spaces between multiple languages:\n",
    "\n",
    ">Supervised: using a train bilingual dictionary (or identical character strings as anchor points), learn a mapping from the source to the target space using (iterative) Procrustes alignment.\n",
    "\n",
    ">Unsupervised: without any parallel data or anchor point, learn a mapping from the source to the target space using adversarial training and (iterative) Procrustes refinement.\n",
    "\n",
    "![Words alignment](https://camo.githubusercontent.com/e8a19eb6772e722fb3fe2cd787e14ed7c4e17ddd/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6172726976616c2f6f75746c696e655f616c6c2e706e67)\n",
    "\n",
    "\n",
    "[Project GitHub](https://github.com/facebookresearch/MUSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings problems\n",
    "\n",
    "1. Word embedding for phrases ('New York', 'North Korea')\n",
    "2. Polysemy - different meaning in different contexts ('river bank', 'financial bank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Further reading\n",
    "\n",
    "0. Yet Another [Article](https://www.shanelynn.ie/get-busy-with-word-embeddings-introduction/) about word embeddings and word2vec.\n",
    "1. Sebastian Ruder's [article](http://ruder.io/word-embeddings-2017/) about recent advances in word embeddings.\n",
    "2. Wordeful [article](https://medium.com/@madrugado/advances-in-nlp-in-2017-part-ii-d8da391a3f01) of Valentin Malykh about other unsupervised methods used in NLP.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
