{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Usupervised learning\n",
    "\n",
    "© Anatolii Stehnii, 2018\n",
    "\n",
    "Sebastian Ruder's [article](http://ruder.io/word-embeddings-2017/).\n",
    "\n",
    "## Lecture 3: Other approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec is a classical example of how deep learning can be used to find word embeddings. However, for practical usage you can consider more novel and sophisticated approaches. In this lecture I will give an overview of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim word2vec implementation\n",
    "\n",
    "First of all, you probably shall not invent a bicycle and write your own word2vec for a practical task. Gensim already did this for you. \n",
    "\n",
    "This model also contains useful tools, like finding most similar word given positive and negative words:\n",
    "```\n",
    ">>> model.wv.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "[('queen', 0.50882536), ...]\n",
    "```\n",
    "\n",
    "Practical example of Word2vec trained on custom corpus can be found in Yurii's Guts [Thrones2Vec](https://github.com/YuriyGuts/thrones2vec) project.\n",
    "\n",
    "[Original paper](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "\n",
    "[Another paper](https://arxiv.org/pdf/1310.4546.pdf)\n",
    "\n",
    "[Project page](https://radimrehurek.com/gensim/models/word2vec.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe: global vector representation\n",
    "\n",
    "This methods, developed in Stanford, also uses a neural model to build an unsupervised probabilistic model of corpus; however, GloVe approach is different. Word2vec model minimize a cross-entropy loss of classifier, which predicts if a word will occur in prescence of another word. GloVe first counts a square matrix of word co-occurance log-probabilities and then tries to optimize word embeddings so dot product of two word vectors is equal to a value in this matrix for this two words. \n",
    "\n",
    "> The training objective of GloVe is to learn word vectors such that their dot product equals the logarithm of the words’ probability of co-occurrence.\n",
    "\n",
    "\n",
    "![GloVe vs Word2vec performance](glove-vs-word2vec.png)\n",
    "\n",
    "[Original paper](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "\n",
    "[Project page](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "[Nice article about topic](https://blog.acolyer.org/2016/04/22/glove-global-vectors-for-word-representation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText\n",
    "\n",
    "This method from Facebook research group trains skip-gram model not for exact words, but for character n-grams. Target vector is a Bag-Of-Grams vector of a context word. For example, word `<where>` can be represented by 3-grams:\n",
    "\n",
    "```\n",
    "<wh\n",
    "whe\n",
    "her\n",
    "ere\n",
    "re>\n",
    "```\n",
    "\n",
    "This approach has multiple advantages:\n",
    "\n",
    "1. Input sparsity reduced.\n",
    "2. Model can capture morphological similarity of words and incorporate this information.\n",
    "3. Vectors for unknown words can be approximated.\n",
    "\n",
    "[Original paper](https://arxiv.org/abs/1607.04606)\n",
    "\n",
    "[Project GitHub](https://github.com/facebookresearch/fastText/#introduction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MUSE\n",
    "\n",
    "This project addressed a problem of multilingual words representations. MUSE decribes methods of aligning of vector spaces between multiple languages:\n",
    "\n",
    ">Supervised: using a train bilingual dictionary (or identical character strings as anchor points), learn a mapping from the source to the target space using (iterative) Procrustes alignment.\n",
    "\n",
    ">Unsupervised: without any parallel data or anchor point, learn a mapping from the source to the target space using adversarial training and (iterative) Procrustes refinement.\n",
    "\n",
    "![Words alignment](https://camo.githubusercontent.com/e8a19eb6772e722fb3fe2cd787e14ed7c4e17ddd/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6172726976616c2f6f75746c696e655f616c6c2e706e67)\n",
    "\n",
    "\n",
    "[Project GitHub](https://github.com/facebookresearch/MUSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings problems\n",
    "\n",
    "1. Word embedding for phrases ('New York', 'North Korea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
