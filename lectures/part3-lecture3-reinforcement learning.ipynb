{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Sequence-to-sequence models\n",
    "\n",
    "© Anatolii Stehnii, 2018\n",
    "\n",
    "## Lecture 3: Reinforcement learning for natural language processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        width:800px;\n",
       "        margin-left:16% !important;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: Helvetica, serif;\n",
       "    }\n",
       "    h4{\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "    div.text_cell_render{\n",
       "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 145%;\n",
       "        font-size: 130%;\n",
       "        width:800px;\n",
       "        margin-left:auto;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "    }\n",
       "    .prompt{\n",
       "        display: None;\n",
       "    }\n",
       "\n",
       "    .text_cell_render h4 {\n",
       "        text-decoration: ;\n",
       "    }\n",
       "\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 22pt;\n",
       "        color: #4057A1;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    \n",
       "    .warning {\n",
       "        color: rgb( 240, 20, 20 )\n",
       "    }\n",
       "\n",
       "    div.warn {\n",
       "        background-color: #fcf2f2;\n",
       "        border-color: #dFb5b4;\n",
       "        border-left: 5px solid #dfb5b4;\n",
       "        padding: 0.5em;\n",
       "    }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"../custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*Returning to the previous lectures: One of the most important problems in NLP is that text representation is **discrete space**. You can not add **noise** to text (or you can – [\n",
    "Adversarial Texts with Gradient Methods, Gong et al., 2018](https://arxiv.org/abs/1801.07175)) and you can not  generate texts with **Generative Adversarial Networks** (or you can – [\n",
    "MaskGAN: Better Text Generation via Filling in the______, Fedus et al., 2018](https://arxiv.org/abs/1801.07736)), because you can not differentiate a text generation function.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement learning for optimization of non-differentiable loss\n",
    "\n",
    "Many of the previously defined NLP problems have special evaluation functions, hand-tailored to these problems, like **BLEU** for Neural Machine Translation or **ROGUE** for Text Summarization. However, the neural network usually trained to maximize **likelihood** function because nor **BLEU** neither **ROGUE** cannot be differentiated, therefore we can't backpropagate an error for them.\n",
    "\n",
    "Reinforcement Learning from its very beginning was a framework for optimization of systems with target function being **unknown or nondifferentiable**. And we can imagine our translation quality function being a delayed reward for actor, which on each step tries to select a proper action *(a next word)* given a state *(previous words and a source sentence)* to maximize this reward. \n",
    "\n",
    "**REINFORCE** – [Simple statistical gradient-following algorithms for connectionist reinforcement learning (Ronald Williams, 1992)](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf), [Sequence Level Training with Recurrent Neural Networks (Ranzato et al., 2015)](https://arxiv.org/abs/1511.06732)\n",
    "\n",
    "**Self-critical sequence training (SCST)** – [A Deep Reinforced Model for Abstractive Summarization (Paulus et al., 2017)](https://arxiv.org/abs/1705.04304), [Self-critical Sequence Training for Image Captioning (Rennie et al., 2017)](https://arxiv.org/abs/1612.00563)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement learning for text generation with GANs\n",
    "\n",
    "![GAN idea](https://pbs.twimg.com/media/CwSKfkBWEAAXd4d.jpg:large)\n",
    "*Via [Chris Olaf](https://twitter.com/ch402/status/793911806494261248/photo/1) twitter*\n",
    "\n",
    "In typical GAN, discriminator compares two pieces of content, real and generated, and learns to distinguish them. A gradient of a function, reversed to discriminator error, backpropagates to generated content and back to generator parameters, optimizing it to generate more realistic content. \n",
    "\n",
    "But text generator function (hardmax, random sampling, beam search) cannot be differentiated. Therefore the usual adversarial approach cannot be used for text generation task (for example, NMT).\n",
    "\n",
    "As been previously said, RL can help to solve a problem with non-differentiable loss function. [Fedus, Goodfellow, and Dai](https://arxiv.org/abs/1801.07736) proposed to train **MaskGAN**: Actor-Critic setup from Reinforcement Learning, where Discriminator network is Critic and Generator network is Actor. However, they generated short sequences and not tested this setup for a text generation of reasonably large amounts. In [Text Generation Based on Generative Adversarial Nets with Latent Variable (Wang et al., 2017)](https://arxiv.org/abs/1712.00170) was used policy gradient to train VGAN - Variational Autoencoder with GAN optimizer.\n",
    "\n",
    "Another GAN approach (without RL, actually): [\n",
    "Long Text Generation via Adversarial Training with Leaked Information (Guo et al., 2017)](https://arxiv.org/abs/1709.08624)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement learning for code generation\n",
    "\n",
    "The previously mentioned **text-to-code** problem usually solved with a neural model, trained with cross-entropy loss and evaluated with metrics, inherited from NMT, like **BLEU**. This raises a problem of training a model, which gives lower credit to a **syntactically and algorithmically correct** result, but of a different form (different order of operations, other variable names, etc).\n",
    "\n",
    "However, another possible supervision and evaluation source can be code execution results: **unit test success** or **database query relevance**. Such supervision cannot be differentiated, but it can be incorporated as a reward and used for REINFORCE or policy gradient training.\n",
    "\n",
    "In [Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning (Zhong et al., 2017)](https://arxiv.org/abs/1709.00103) this approach is used to address generation of queries to WikiSQL database from query natural description; in addition to cross-entropy objective function, reward signal from an execution of generation results is used to learn a policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
