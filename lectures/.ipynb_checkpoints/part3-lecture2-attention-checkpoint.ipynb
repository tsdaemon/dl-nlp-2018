{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Sequence-to-sequence models\n",
    "\n",
    "© Anatolii Stehnii, 2018\n",
    "\n",
    "## Lecture 2: Neural attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "MathJax.Hub.Queue(\n",
       "  [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n",
       "  [\"PreProcess\", MathJax.Hub],\n",
       "  [\"Reprocess\", MathJax.Hub]\n",
       ");"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Queue(\n",
    "  [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n",
    "  [\"PreProcess\", MathJax.Hub],\n",
    "  [\"Reprocess\", MathJax.Hub]\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        width:800px;\n",
       "        margin-left:16% !important;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: Helvetica, serif;\n",
       "    }\n",
       "    h4{\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "    div.text_cell_render{\n",
       "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 145%;\n",
       "        font-size: 130%;\n",
       "        width:800px;\n",
       "        margin-left:auto;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "    }\n",
       "    .prompt{\n",
       "        display: None;\n",
       "    }\n",
       "\n",
       "    .text_cell_render h4 {\n",
       "        text-decoration: ;\n",
       "    }\n",
       "\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 22pt;\n",
       "        color: #4057A1;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    \n",
       "    .warning {\n",
       "        color: rgb( 240, 20, 20 )\n",
       "    }\n",
       "\n",
       "    div.warn {\n",
       "        background-color: #fcf2f2;\n",
       "        border-color: #dFb5b4;\n",
       "        border-left: 5px solid #dfb5b4;\n",
       "        padding: 0.5em;\n",
       "    }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"../custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with sentence vector\n",
    "\n",
    "Theoretically, a sufficiently large encoder-decoder model should be able to perform the machine translation perfectly. However, to encode all words and their dependencies in the arbitrary-length sentences, the thought vector should have\n",
    "enormous length. Such model would require massive computational resources to train and to use, thus this approach is ineffective.\n",
    "\n",
    "This problem can be solved with attention technique ([Bahdanau et al., 2014](https://arxiv.org/abs/1409.0473)). Its basic idea is to replace a single vector representation of the input sentence with references to representations of different words in it.\n",
    "\n",
    "![Attention](attention.png)\n",
    "*Attention matrix for English to French translation. Via [WildML](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math\n",
    "\n",
    "Matrix $\\textbf{H}_\\textbf{x} \\in \\mathbb{R}^{n\\times m}$ ($n$ – number of encoding steps, $m$ – hidden vector dimensionality) is created from hidden vectors $\\textbf{h}_{x}^{(t)}$ from each step of encoding process.\n",
    "\n",
    "During decoding, each input vector $\\textbf{w}_{y}^{(t)}$ is concatenated with a context vector $\\boldsymbol{\\phi}^{(t)}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{h}_y^{(t)} = rnn([\\textbf{w}_y^{(t)}, \\boldsymbol{\\phi}^{(t)}], \\textbf{h}_y^{(t-1)})\n",
    "\\end{equation}\n",
    "\n",
    "Context vector $\\phi^{(t)}$ is calculated as a weighted sum of all encoder representations:\n",
    "\n",
    "\\begin{equation}\n",
    "\\boldsymbol{\\phi}^{(t)} = \\textbf{H}_{\\textbf{x}}^{T}\\cdot\\boldsymbol{\\alpha}^{(t)}\n",
    "\\end{equation}\n",
    "\n",
    "Weights for the attention vector $\\alpha^{(t)}$ can be calculated with an arbitrary attention score function (for example, vector product) for all pairs of the decoder vector $h_y^{(t-1)}$ and the encoder vector $h_x^{(i)}, \\forall i \\in 1..n$.\n",
    "\n",
    "In original work of Bahdanau et al. were used DNN with one hidden layer:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{gathered}\n",
    "\\hat{\\boldsymbol{\\alpha}}_i^{(t)} = \\textbf{W}_{attn1} \\cdot [\\textbf{h}_x^{(i)}, \\textbf{h}_y^{(t-1)}] \\\\\n",
    "\\alpha_i^{(t)} = \\textbf{W}_{attn2} \\cdot tanh(\\hat{\\boldsymbol{\\alpha}}_{i}^{(t)}) \\\\\n",
    "\\boldsymbol{\\alpha}^{(t)} = softmax(\\boldsymbol{\\alpha}^{(t)})\n",
    "\\end{gathered}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing an idea\n",
    "\n",
    "1. **Attention over history**\n",
    "    \n",
    "    Along with attention over encoder steps, you can use attention over previous decoder steps to recover possible lost information.\n",
    "    \n",
    "1. **Attention over context**\n",
    "\n",
    "    In a previously mentioned [MQAN](https://einstein.ai/research/the-natural-language-decathlon) network along with source and target also used context - additional information, which can be used by network to find answer to question.\n",
    "    \n",
    "2. **Attention is all you need** - [Vaswani et al., 2017](http://papers.nips.cc/paper/7181-attention-is-all-you-need)\n",
    "    \n",
    "    Actually, when using attention over history, you can throw out all RNN and LSTM stuff and just model any sequence with attention. In paper above, this approach demonstrated better results for machine translation task with lower training time.\n",
    "    \n",
    "3. **Show, attend and tell** - [Xu et al., 2015](http://arxiv.org/abs/1502.03044)\n",
    "\n",
    "    Here attention over images is used to generate image description:\n",
    "    \n",
    "    ![attention image](attention-image.png)\n",
    "    \n",
    "4. **Attention over memory** \n",
    "    \n",
    "    Hidden state of a recurrent neural network is a basic example of internal memory. LSTM developed this context, allowing to store information about longer sequences. Soft attention allows network to interacts with even larger amounts of hierarchical data in [End-To-End memory networks](https://arxiv.org/abs/1503.08895)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointer networks\n",
    "\n",
    "A neural network operates with vector representations of words that are selected from a predefined vocabulary. This imposes the problem of unknown words that don't have a vector representation. This is especially important for the translation task where both an input and an output sequences could contain rare, special words or names.\n",
    "\n",
    "However, names of people or locations should not be translated but copied to a target sequence. [Vinyals et al., 2015](http://papers.nips.cc/paper/5866-pointer-networks.pdf) proposed a solution of this problem with a **pointer network**. For each decoding step it calculates the probability of the next word to be copied from the input sequence. Calculation of this probability is described below step-by-step.\n",
    "\n",
    "Let's denote $\\textbf{h}_{y}^{(t)}$ – as a decoder output vector on decoding step $t$ and $\\textbf{h}_{x}^{(i)}$ – as an encoder output vector on decoding step. First a hidden state of the pointer network is calculated:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{gathered}\n",
    "\\textbf{h}_{x.pointer}^{(i)} = \\textbf{W}_{x}^T \\cdot \\textbf{h}_x^{(i)}\\\\\n",
    "\\textbf{h}_{y.pointer}^{(t)} = \\textbf{W}_{y}^T \\cdot \\textbf{h}_y^{(t)} \\\\\n",
    "\\textbf{h}_{pointer}^{(t, i)} = tanh(\\textbf{h}_{x.pointer}^{(i)} + \\textbf{h}_{y.pointer}^{(t)})\\\\\n",
    "\\end{gathered}\n",
    "\\end{equation}\n",
    "\n",
    "This is performed for all $\\textbf{h}_{x}^{(i)}, i \\in 1..n$. Vectors $\\textbf{h}_{pointer}^{(t, i)}$ are combined into matrix $\\textbf{H}_{pointer}^{(t)}$. This matrix is translsted to vector of probabilities for each word to be copied from a source sequence into target:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{gathered}\n",
    "\\textbf{h}^{(t)}_{copy} = \\textbf{W}_{pointer}^T \\cdot \\textbf{H}^{(t)}_{pointer} \\\\\n",
    "\\textbf{p}_{copy}^{(t)} = softmax(\\textbf{h}^{(t)}_{copy})\\\\\n",
    "\\end{gathered}\n",
    "\\end{equation}\n",
    "\n",
    "The vector $P^{(t)}$ contains a probability for each input sequence token to be copied into the output sequence on step $t$. This way decoder generation should be performed in two steps. First, a probability of generating or copying is infered from hidden state $\\textbf{h}_y^{(t)}$ using matrix $\\textbf{W}_{g.or.c} \\in \\mathbb{R}^{m\\times2}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{p}_{g.or.c}^{(t)} = softmax(\\textbf{W}_{g.or.c}^T \\cdot \\textbf{h}_y^{(t)})\n",
    "\\end{equation}\n",
    "\n",
    "$\\textbf{p}_{g.or.c}^{(t)}$ is a vector of two elements – the first is a probability of a generating a word and second is a probability of a copying a word. This way model results in a conditional probaility of two actions:\n",
    "\n",
    "\\begin{equation}\n",
    "p(y_t|y_{<t}, \\textbf{x}) = p(y_t=w_y|y_{<t},\\textbf{x}) \\cdot p_t(y_t=generate|y_{<t},\\textbf{x}) + p(y_t=w_x|y_{<t},\\textbf{x}) \\cdot p_t(y_t=copy|y_{<t},\\textbf{x})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
