{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Sequence-to-sequence models\n",
    "\n",
    "© Anatolii Stehnii, 2018\n",
    "\n",
    "## Lecture 1: Neural machine translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "MathJax.Hub.Queue(\n",
       "  [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n",
       "  [\"PreProcess\", MathJax.Hub],\n",
       "  [\"Reprocess\", MathJax.Hub]\n",
       ");"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Queue(\n",
    "  [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n",
    "  [\"PreProcess\", MathJax.Hub],\n",
    "  [\"Reprocess\", MathJax.Hub]\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        width:800px;\n",
       "        margin-left:16% !important;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: Helvetica, serif;\n",
       "    }\n",
       "    h4{\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "    div.text_cell_render{\n",
       "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 145%;\n",
       "        font-size: 130%;\n",
       "        width:800px;\n",
       "        margin-left:auto;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "    }\n",
       "    .prompt{\n",
       "        display: None;\n",
       "    }\n",
       "\n",
       "    .text_cell_render h4 {\n",
       "        text-decoration: ;\n",
       "    }\n",
       "\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 22pt;\n",
       "        color: #4057A1;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    \n",
       "    .warning {\n",
       "        color: rgb( 240, 20, 20 )\n",
       "    }\n",
       "\n",
       "    div.warn {\n",
       "        background-color: #fcf2f2;\n",
       "        border-color: #dFb5b4;\n",
       "        border-left: 5px solid #dfb5b4;\n",
       "        padding: 0.5em;\n",
       "    }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"../custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical overview\n",
    "\n",
    "![X-ray glasses](https://cdn-images-1.medium.com/max/2000/1*JE5UQE0Jo5f7RM-TCkMpSg.png)\n",
    "*Via [Medium](https://medium.com/beluga-team/a-brief-and-untold-history-of-machine-translation-ea7dc1aa1f5)*\n",
    "\n",
    "**1954** – First demonstration of machine translation in IBM, 49 sentences translated from Russian to English.\n",
    "\n",
    "**1956** – Dartmouth Conference, term artificial intelligence used for machine translation task.\n",
    "\n",
    "![History](https://cdn-images-1.medium.com/max/2000/1*d-iF6wcVYCWFDLkghpJvkw.png)\n",
    "*Via [Medium](https://medium.freecodecamp.org/a-history-of-machine-translation-from-the-cold-war-to-deep-learning-f1d335ce8b5)*\n",
    "\n",
    "**1966** – ALPAC report, criticised machine translation efforts and recommended the need for basic research in computational linguistics.\n",
    "\n",
    "**1990-2015** – era of Statistical Machine Translation. Parallel bilingual corpora are used to build statistical model of $p(e|f)$ - probability that sentence $e$ in target language is a translation of sentence $f$ in source language.\n",
    "\n",
    "More details about old times in [A History of Machine Translation From the Cold Ward to Deep Learning](https://medium.freecodecamp.org/a-history-of-machine-translation-from-the-cold-war-to-deep-learning-f1d335ce8b5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main problems of SMT**:\n",
    "1. Different word meaning in different context (polysemy)\n",
    "1. Realigning words in target sentece\n",
    "2. Transfer of syntactical structure\n",
    "3. Rare words and named entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2014** – *[Cho et al](https://arxiv.org/abs/1406.1078)* \"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\" - first encoder-decoder paper.\n",
    "\n",
    "**2016** - Google launched [GNMT for 9 languages](https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html). 8 levels encoder + 8 levels decoder + attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMT task\n",
    "\n",
    "Formulation of machine translation problem:\n",
    "\n",
    "Given a sentence in source language $\\textbf{x}$, the task is to infer a sentence in target language $\\textbf{y}$ which maximizes conditional probability $p(\\textbf{y}|\\textbf{x})$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{y} = \\underset{\\hat{\\textbf{y}}}{\\mathrm{argmax}} p(\\hat{\\textbf{y}}|\\textbf{x})\n",
    "\\end{equation}\n",
    "\n",
    "In NMT we approximate $p(\\textbf{y}|\\textbf{x})$ using neural model with parameters $\\theta$. To learn values of $\\theta$, we use set of training examples, which consist of tuples $(\\textbf{y}^{(x)}, \\textbf{x})$. Model parameters are learned by maximizing conditional log-probabilities of the training set:\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta = \\underset{\\theta}{\\mathrm{argmax}} \\sum_{\\textbf{y}^{(x)}, \\textbf{x}} log p(\\textbf{y}^{(x)}| \\textbf{x}; \\theta)\n",
    "\\end{equation}\n",
    "\n",
    "Since sentences consist of words, we can factorize this probability on separate word probabilities. Let's denote target sentece $\\textbf{y}$ as ordered set of words $y_1, y_2, \\ldots ,y_{t-1}, y_t$. Then probability $p(\\textbf{y}|\\textbf{x})$ can rewritten:\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\textbf{y}|\\textbf{x}) = \\prod_{i=1}^{t} p(y_i|y_{i-1}, y_{i-2}, \\ldots, y_{2}, y_{1}, \\textbf{x})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural model\n",
    "\n",
    "We can approximate probability $p(y_i|y_{i-1}, y_{i-2}, \\ldots, y_{2}, y_{1}, \\textbf{x})$ using neural model. All we need is to somehow represent information about source sentece  $\\textbf{x}$ and previous words $y_{i-1}, y_{i-2}, \\ldots, y_{2}, y_{1}$ as vectors. This can be done with a *Sequence-to-sequence* model:\n",
    "\n",
    "1. Source sentence $\\textbf{x}$ transformed to a vector $\\textbf{h}_{\\textbf{x}}$ by RNN called encoder. \n",
    "2. Each word from target sentece ($y_i$) decoded by another RNN (decoder). Decoder hidden state $\\textbf{h}_{y}^{(i-1)}$ represents information about previous words $y_{i-1}, y_{i-2}, \\ldots, y_{2}, y_{1}$.\n",
    "3. Two ways to connect encoder and decoder: \n",
    "    * Thought vector – last hidden state of encoder $\\textbf{h}_{x}^{(n)}$ used as initial hidden state for decoder $\\textbf{h}_{y}^{(0)}$.\n",
    "    * Attention – decoder RNN concats its input $\\textbf{w}_{y}^{(i)}$ with attention vector $\\phi_{y}^{(i)}$, which is a composition of encoder hidden states $\\textbf{H}_{\\textbf{x}}$. \n",
    "    \n",
    "    *Attention will be described in details in the next lecture.*\n",
    "    \n",
    "![Seq2seq with thought vector](Seq2seq thought vector.png)\n",
    "*Seq2seq with thought vector*\n",
    "\n",
    "![Seq2seq with attention](Seq2seq-attention.png)\n",
    "*Seq2seq with attention*\n",
    "\n",
    "Basically, thought vector approach is not used anymore, since it is clear, that it is impossible to embed all information about a variable size sequence into a fixed size vector. Attention or its modifications are mostly used in all modern Seq2seq networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infering result\n",
    "\n",
    "To get distribution of possible words for a decoding step $i$, you can transform decoder output $\\textbf{h}_{y}^{(i)}$ to probability distribution of possible words:\n",
    "\n",
    "\\begin{equation}\n",
    "p(y_i|y_{<i}, \\textbf{x}) = softmax(\\textbf{W}_{out}^T \\cdot \\textbf{h}_{y}^{(i)})\n",
    "\\end{equation}\n",
    "\n",
    "To infer whole sentence, using this distribution, you can choose different strategies:\n",
    "\n",
    "1. **Greedy search** – select a word with the highest probability.\n",
    "2. **Random search** – sample a random word from the distribution.\n",
    "3. **Beam search** – grow a tree structure of possible options, prunning it to $n$ most possible options at each time step.\n",
    "\n",
    "![Beam search](beam-search.png)\n",
    "*Beam search example (via [Google AI Blog](https://ai.googleblog.com/2016/05/chat-smarter-with-allo.html))*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "As mentioned before, a model is trained using log-likelihod loss, which is basically a sum of  logarithms of probabilities of each word in a train target example.\n",
    "\n",
    "\\begin{equation}\n",
    "log p(\\textbf{y}|\\textbf{x}) = \\sum_{i=1}^{t} log p(y_i|y_{<i}, \\textbf{x})\n",
    "\\end{equation}\n",
    "\n",
    "During infering, each decoder input is its previous output. To simplify training process, you can use train target example as decoder input. This is called **teacher forcing**. But this approach results in slightly different distibutions output during training and during inference. **Professor forcing** approach ([Goyal et al, 2016](http://papers.nips.cc/paper/6098-professor-forcing-a-new-algorithm-for-training-recurrent-networks)) proposes to use additional adversarial network, which is trained to distinguish distributions obtained during network training with **teacher forcing** and distributions obtained during infering without ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other applications of Seq2seq\n",
    "\n",
    "\n",
    "1. **Text summarization**\n",
    "    \n",
    "    Seq2seq can be trained to obtain short summaries of news or abstracts of scientific papers. However, huge search spaces of both input and output requires modification of attention (for example, infra-temporal attention, paragraph vectors, etc).\n",
    "    \n",
    "    GitHub repository with overview of text summarization approaches: https://github.com/icoxfog417/awesome-text-summarization\n",
    "\n",
    "2. **Question answering, natural database interfaces**\n",
    "\n",
    "3. **Text-to-code, code-to-text translation **\n",
    "    \n",
    "    Use code and it's comments ([Barone and Sennrich 2017](https://arxiv.org/abs/1707.02275), [Yin and Neubig 2017](https://arxiv.org/abs/1704.01696)) to generate code from it's comments and vice versa. Non practical, but quite funny."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "1. **BLEU** (bilingual evaluation understudy) – classical metric for MT evaluation. Can use multiple reference translations. Established as the most correlated with human evaluation.\n",
    "\n",
    "2. **ROGUE** (Recall-Oriented Understudy for Gisting Evaluation) — for text summarization.\n",
    "    1. **CROGUE** — incorporate word embeddings to advocate for semantically similar but rephrased summaries (Zaytsev et al, 2018). C stands for \"continuous\".\n",
    "\n",
    "3. **METEOR** (Metric for Evaluation of Translation with Explicit ORdering) – another metric for translations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2tree, Tree2tree\n",
    "\n",
    "[Yin and Neubig 2017](https://arxiv.org/abs/1704.01696) addressed Seq2tree architecture in a task of natural language translation to code. Syntactically correct programming code can be represented as **abstract syntax tree**, therefore to generate correct program neural network can infer tree structures instead of sequences. To do this, tree generation is decomposed to sequence of tree-growing and terminal commands. Additionaly, recurrent network used as input concatenation of previous hidden state with a hidden state for root node.\n",
    "\n",
    "[Stehnii 2017](http://er.ucu.edu.ua/handle/1/1191?locale-attribute=en) complemented this architecture with recursive encoder, which used as input natural description, parsed to dependency trees and semantic trees. Resulting Tree2tree network has not improved performance, but improved author self-estimate, at least.\n",
    "\n",
    "[Chen et al 2017](https://arxiv.org/abs/1707.05436) also incorporated syntactic structures for machine translation with syntactic attention and bi-directional syntactic encoder. Their network outperformed current state-of-the art for Chinese-English translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge transfer\n",
    "\n",
    "#### Google multylanguage translation\n",
    "\n",
    "![Google translation](google-multy.gif)\n",
    "*Via [Google AI Blog](https://ai.googleblog.com/2016/11/zero-shot-translation-with-googles.html)*\n",
    "\n",
    "#### Multitask Question Answering Network - MQAN\n",
    "\n",
    "One network to ~~rule them all~~ [solve 10 tasks](https://einstein.ai/research/the-natural-language-decathlon):\n",
    "\n",
    "![MQAN](MQAN.gif)\n",
    "\n",
    "1. **Question Answering**\n",
    "\n",
    "2. **Machine Translation**\n",
    "\n",
    "3. **Text Summarization**\n",
    "\n",
    "4. **Natural Language Inference**: models receive two input sentences: a premise and a hypothesis. Models must then classify the inference relationship between the premise and hypothesis as one of entailment, neutrality, or contradiction.\n",
    "\n",
    "5. **Sentiment Analysis**\n",
    "\n",
    "6. **Relation Extraction**\n",
    "\n",
    "7. **Goal-Oriented Dialogue**: based on user utterances and system actions, dialogue state trackers keep track of which predefined goals the user has for the dialogue system and which kinds of requests the user makes as the system and user interact turn-by-turn.\n",
    "\n",
    "8. **Semantic Parsing (SQL query generation)**\n",
    "\n",
    "9. **Pronoun Resolution**: \"Joan made sure to thank Susan for the help she had [given/received]. Who had [given/received] help? Susan or Joan?\"\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
