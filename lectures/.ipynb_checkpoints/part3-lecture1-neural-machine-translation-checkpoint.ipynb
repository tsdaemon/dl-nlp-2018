{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Sequence-to-sequence models\n",
    "\n",
    "© Anatolii Stehnii, 2018\n",
    "\n",
    "## Lecture 1: Neural machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical overview\n",
    "\n",
    "![X-ray glasses](https://cdn-images-1.medium.com/max/2000/1*JE5UQE0Jo5f7RM-TCkMpSg.png)\n",
    "*Via [Medium](https://medium.com/beluga-team/a-brief-and-untold-history-of-machine-translation-ea7dc1aa1f5)*\n",
    "\n",
    "**1954** – First demonstration of machine translation in IBM, 49 sentences translated from Russian to English.\n",
    "\n",
    "**1956** – Dartmouth Conference, term artificial intelligence used for machine translation task.\n",
    "\n",
    "![History](https://cdn-images-1.medium.com/max/2000/1*d-iF6wcVYCWFDLkghpJvkw.png)\n",
    "*Via [Medium](https://medium.freecodecamp.org/a-history-of-machine-translation-from-the-cold-war-to-deep-learning-f1d335ce8b5)*\n",
    "\n",
    "**1966** – ALPAC report, criticised machine translation efforts and recommended the need for basic research in computational linguistics.\n",
    "\n",
    "**1990-2015** – era of Statistical Machine Translation. Parallel bilingual corpora are used to build statistical model of $p(e|f)$ - probability that sentence $e$ in target language is a translation of sentence $f$ in source language.\n",
    "\n",
    "More details about old times in [A History of Machine Translation From the Cold Ward to Deep Learning](https://medium.freecodecamp.org/a-history-of-machine-translation-from-the-cold-war-to-deep-learning-f1d335ce8b5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main problems of SMT**:\n",
    "1. Different word meaning in different context (polysemy)\n",
    "1. Realigning words in target sentece\n",
    "2. Transfer of syntactical structure\n",
    "3. Rare words and named entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2014** – *[Cho et al](https://arxiv.org/abs/1406.1078)* \"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\" - first encoder-decoder paper.\n",
    "\n",
    "**2016** - Google launched [GNMT for 9 languages](https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html). 8 levels encoder + 8 levels decoder + attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMT task\n",
    "\n",
    "Formulation of machine translation problem:\n",
    "\n",
    "Given a sentence in source language $\\textbf{x}$, the task is to infer a sentence in target language $\\textbf{y}$ which maximizes conditional probability $p(\\textbf{y}|\\textbf{x})$:\n",
    "\n",
    "$$\n",
    "\\textbf{y} = \\underset{\\hat{\\textbf{y}}}{\\mathrm{argmax}} p(\\hat{\\textbf{y}}|\\textbf{x})\n",
    "$$\n",
    "\n",
    "In NMT we approximate $p(\\textbf{y}|\\textbf{x})$ using neural model with parameters $\\theta$. To learn values of $\\theta$, we use set of training examples, which consist of tuples $(\\textbf{y}^{(x)}, \\textbf{x})$. Model parameters are learned by maximizing conditional log-probabilities of the training set:\n",
    "\n",
    "$$\n",
    "\\theta = \\underset{\\theta}{\\mathrm{argmax}} \\sum_{\\textbf{y}^{(x)}, \\textbf{x}} log p(\\textbf{y}^{(x)}| \\textbf{x}; \\theta)\n",
    "$$\n",
    "\n",
    "Since sentences consist of words, we can factorize this probability on separate word probabilities. Let's denote target sentece $\\textbf{y}$ as ordered set of words $y_1, y_2, \\ldots ,y_{t-1}, y_t$. Then probability $p(\\textbf{y}|\\textbf{x})$ can rewritten:\n",
    "\n",
    "$$\n",
    "p(\\textbf{y}|\\textbf{x}) = \\prod_{i=1}^{t} p(y_i|y_{i-1}, y_{i-2}, \\ldots, y_{2}, y_{1}, \\textbf{x})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural model\n",
    "\n",
    "We can approximate probability $p(y_i|y_{i-1}, y_{i-2}, \\ldots, y_{2}, y_{1}, \\textbf{x})$ using neural model. All we need is to somehow represent information about source sentece  $\\textbf{x}$ and previous words $y_{i-1}, y_{i-2}, \\ldots, y_{2}, y_{1}$ as vectors. This can be done with a *Sequence-to-sequence* model:\n",
    "\n",
    "1. Source sentence $\\textbf{x}$ transformed to a vector $h_{\\textbf{x}}$ by RNN called encoder. \n",
    "2. Each word from target sentece ($y_i$) decoded by another RNN (decoder). Decoder hidden state $h_{y_{i-1}}$ represents information about previous words $y_{i-1}, y_{i-2}, \\ldots, y_{2}, y_{1}$.\n",
    "3. Two ways to connect encoder and decoder: \n",
    "    * Thought vector – last hidden state of encoder $h_{x_n}$ used as initial hidden state for decoder $h_{y_0}$.\n",
    "    * Attention – decoder RNN concats hidden state $h_{y_{i-1}}$ with attention vector $a_{y_{i}}$, which is a composition of encoder hidden states $h_{x_k}$. Attention will be described in details in the next lecture.\n",
    "    \n",
    "![Seq2seq with tought vector](Seq2seq thought vector.png)\n",
    "*Seq2seq with tought vector*\n",
    "\n",
    "![Seq2seq with attention](Seq2seq-attention.png)\n",
    "*Seq2seq with attention*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result search\n",
    "1. Greedy search\n",
    "2. Random search\n",
    "3. Beam search https://ai.googleblog.com/2016/05/chat-smarter-with-allo.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other applications of Seq2seq models\n",
    "\n",
    "Text summarization\n",
    "Question answering\n",
    "Text-to-code, code-to-text translation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented networks\n",
    "seq2tree, tree2tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge transfer\n",
    "\n",
    "How Google is using different encoders and decoders\n",
    "\n",
    "#### Multitask Question Answering Network - MQAN\n",
    "\n",
    "One network to ~~rule them all~~ solve 10 tasks:\n",
    "1. **Question Answering**\n",
    "\n",
    "2. **Machine Translation**\n",
    "\n",
    "3. **Text Summarization**\n",
    "\n",
    "4. **Natural Language Inference**: models receive two input sentences: a premise and a hypothesis. Models must then classify the inference relationship between the premise and hypothesis as one of entailment, neutrality, or contradiction.\n",
    "\n",
    "5. **Sentiment Analysis**\n",
    "\n",
    "6. **Relation Extraction**\n",
    "\n",
    "7. **Goal-Oriented Dialogue**: based on user utterances and system actions, dialogue state trackers keep track of which predefined goals the user has for the dialogue system and which kinds of requests the user makes as the system and user interact turn-by-turn.\n",
    "\n",
    "8. **Semantic Parsing (SQL query generation)**\n",
    "\n",
    "9. **Pronoun Resolution**: \"Joan made sure to thank Susan for the help she had [given/received]. Who had [given/received] help? Susan or Joan?\"\n",
    "\n",
    "\n",
    "https://einstein.ai/research/the-natural-language-decathlon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        width:800px;\n",
       "        margin-left:16% !important;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: Helvetica, serif;\n",
       "    }\n",
       "    h4{\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "    div.text_cell_render{\n",
       "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 145%;\n",
       "        font-size: 130%;\n",
       "        width:800px;\n",
       "        margin-left:auto;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "    }\n",
       "    .prompt{\n",
       "        display: None;\n",
       "    }\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 22pt;\n",
       "        color: #4057A1;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    \n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"../custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
