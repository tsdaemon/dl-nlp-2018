{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Usupervised learning\n",
    "\n",
    "© Anatolii Stehnii, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/anatolii.stehnii/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env LC_ALL=en_US.UTF-8\n",
    "%env LANG=en_US.UTF-8\n",
    "\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import nltk\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric representation for words\n",
    "\n",
    "Numerical data naturally have a meaning for computational machine. Numbers can be **compared**, a **distance** between vectors can be measured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5156862774427124"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 5, 6]\n",
    "b = [-2, -6, 1]\n",
    "distance = spatial.distance.cosine(a, b)\n",
    "distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this can not be easy done for **words**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = ['weather', 'is', 'good']\n",
    "b = ['sun', 'is', 'shining']\n",
    "distance = None # ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words are **discreete symbols** and can be encoded as **one-hot-vector**, sentences can be encoded as a **bag of words**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW vector for `weather is good`: [1 1 1 0 0]\n",
      "BoW vector for `sun is shining`: [0 1 0 1 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66666666666666663"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = {\n",
    "    'weather': [1, 0, 0, 0, 0],\n",
    "    'is': [0, 1, 0, 0, 0],\n",
    "    'good': [0, 0, 1, 0, 0],\n",
    "    'sun': [0, 0, 0, 1, 0],\n",
    "    'shining': [0, 0, 0, 0, 1],\n",
    "}\n",
    "a = np.sum(list(map(lambda w: vectors[w], ['weather', 'is', 'good'])), axis=0)\n",
    "print('BoW vector for `weather is good`: {}'.format(a))\n",
    "b = np.sum(list(map(lambda w: vectors[w], ['sun', 'is', 'shining'])), axis=0)\n",
    "print('BoW vector for `sun is shining`: {}'.format(b))\n",
    "\n",
    "distance = spatial.distance.cosine(a, b)\n",
    "distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But for the whole vocabulary, we will have 500,000-dimensional vectors. Also, this approach totally ignores words **meanings, relations, and similarity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meaning\n",
    "Definition of **meaning**:\n",
    " 1. the logical connotation of a word or phrase;\n",
    " 2. what is intended to be, or actually is, expressed or indicated; signification;\n",
    " 3. the thing that is conveyed especially by language.\n",
    " \n",
    "Meaning of a **natural language** is a complex problem. The same words can have a different meaning in different contexts (**polysemy**). Different words also can have a similar meaning (**synonymy**). Words with broader meaning can include meaning for more specific categories (**hypernymy and hyponymy**).\n",
    "\n",
    "The simple solution for meaning representation is to manually mark up a graph of relations between words (**WordNet**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "def synset_to_str(synset):\n",
    "    return '({}) {}'.format(synset.pos(), ', '.join(map(str, synset.lemma_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n) evil, immorality, wickedness, iniquity\n",
      "(n) evil\n",
      "(n) evil, evilness\n",
      "(a) evil\n",
      "(s) evil, vicious\n",
      "(s) malefic, malevolent, malign, evil\n"
     ]
    }
   ],
   "source": [
    "# synonyms\n",
    "for synset in wn.synsets('evil'):\n",
    "    print(synset_to_str(synset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n) feline, felid\n",
      "(n) carnivore\n",
      "(n) placental, placental_mammal, eutherian, eutherian_mammal\n",
      "(n) mammal, mammalian\n",
      "(n) vertebrate, craniate\n",
      "(n) chordate\n",
      "(n) animal, animate_being, beast, brute, creature, fauna\n",
      "(n) organism, being\n",
      "(n) living_thing, animate_thing\n",
      "(n) whole, unit\n",
      "(n) object, physical_object\n",
      "(n) physical_entity\n",
      "(n) entity\n"
     ]
    }
   ],
   "source": [
    "# hypernyms\n",
    "hypernyms = lambda s: s.hypernyms()\n",
    "cat = wn.synset('cat.n.01')\n",
    "for synset in list(cat.closure(hypernyms)):\n",
    "    print(synset_to_str(synset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n) domestic_cat, house_cat, Felis_domesticus, Felis_catus\n",
      "(n) wildcat\n",
      "(n) Abyssinian, Abyssinian_cat\n",
      "(n) alley_cat\n",
      "(n) Angora, Angora_cat\n",
      "(n) Burmese_cat\n",
      "(n) Egyptian_cat\n",
      "(n) kitty, kitty-cat, puss, pussy, pussycat\n",
      "(n) Maltese, Maltese_cat\n",
      "(n) Manx, Manx_cat\n",
      "(n) mouser\n",
      "(n) Persian_cat\n",
      "(n) Siamese_cat, Siamese\n",
      "(n) tabby, tabby_cat\n",
      "(n) tabby, queen\n",
      "(n) tiger_cat\n",
      "(n) tom, tomcat\n",
      "(n) tortoiseshell, tortoiseshell-cat, calico_cat\n",
      "(n) cougar, puma, catamount, mountain_lion, painter, panther, Felis_concolor\n",
      "(n) European_wildcat, catamountain, Felis_silvestris\n",
      "(n) jaguarundi, jaguarundi_cat, jaguarondi, eyra, Felis_yagouaroundi\n",
      "(n) jungle_cat, Felis_chaus\n",
      "(n) kaffir_cat, caffer_cat, Felis_ocreata\n",
      "(n) leopard_cat, Felis_bengalensis\n",
      "(n) lynx, catamount\n",
      "(n) manul, Pallas's_cat, Felis_manul\n",
      "(n) margay, margay_cat, Felis_wiedi\n",
      "(n) ocelot, panther_cat, Felis_pardalis\n",
      "(n) sand_cat\n",
      "(n) serval, Felis_serval\n",
      "(n) tiger_cat, Felis_tigrina\n",
      "(n) blue_point_Siamese\n",
      "(n) gib\n",
      "(n) bobcat, bay_lynx, Lynx_rufus\n",
      "(n) Canada_lynx, Lynx_canadensis\n",
      "(n) caracal, desert_lynx, Lynx_caracal\n",
      "(n) common_lynx, Lynx_lynx\n",
      "(n) spotted_lynx, Lynx_pardina\n"
     ]
    }
   ],
   "source": [
    "hyponyms = lambda s: s.hyponyms()\n",
    "for synset in list(cat.closure(hyponyms)):\n",
    "    print(synset_to_str(synset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distance - shortest path in hyponyms/hypernims graph between two words\n",
    "dog = wn.synset('dog.n.01')\n",
    "dog.shortest_path_distance(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other WordNet features: http://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "WordNet is interesting as a word catalogue, but it has some **major issues**:\n",
    "1. whole **language complexity** cannot be inferred only from synonymy, hyponymy, and hypernymy relations;\n",
    "2. WordNet is missing new words, it requires **manual labor** to adapt, and it is prone to **human errors**;\n",
    "3. word similarity is **not accurate**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution \n",
    "\n",
    "Map words into n-dimensional space, where relations between words will be encoded into **spatial words positions**.\n",
    "\n",
    "### Core idea\n",
    "\n",
    "A word’s meaning is given by the words that frequently appear close-by.\n",
    "\n",
    "*\"You shall know a word by the company it keeps\" (John Rupert Firth. 1957:11)*\n",
    "\n",
    "**Distributional hypothesis**: linguistic items with similar distributions have similar meanings. https://en.wikipedia.org/wiki/Distributional_semantics\n",
    "\n",
    "To find a representation of a word $w$ we need to use it's **context**: a set of words, which occured nearby is some window (for example, paragraph).\n",
    "\n",
    "## Latent semantic analysis\n",
    "\n",
    "You can form term-document matrix (TF-IDF, BoW) to describe a words context; it is a sparse matrix $\\textbf{X}$ where terms are rows and documents are columns:\n",
    "\n",
    "$$\n",
    "\\begin{matrix} \n",
    " & \\textbf{d}_j \\\\\n",
    " & \\downarrow \\\\\n",
    "\\textbf{t}_i^T \\rightarrow &\n",
    "\\begin{bmatrix} \n",
    "x_{1,1} & \\dots & x_{1,j} & \\dots & x_{1,n} \\\\\n",
    "\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{i,1} & \\dots & x_{i,j} &  \\dots & x_{i,n} \\\\\n",
    "\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{m,1} & \\dots & x_{m,j} & \\dots & x_{m,n} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "After the matrix formed, any **rank lowering method** (PCA, SVD) can be used to reduce dimensionality and extract main components for terms (or for documents, if you task is to project documents into the linear space):\n",
    "\n",
    "$$\n",
    "\\textbf{X} = \\textbf{U} \\textbf{S} \\textbf{V}^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{matrix} \n",
    " & X & & & U & & \\textbf{S} & & V^T \\\\\n",
    " & (\\textbf{d}_j) & & & & & & & (\\hat{\\textbf{d}}_j) \\\\\n",
    " & \\downarrow & & & & & & & \\downarrow \\\\\n",
    "(\\textbf{t}_i^T) \\rightarrow \n",
    "&\n",
    "\\begin{bmatrix}\n",
    "x_{1,1} & \\dots & x_{1,j} & \\dots & x_{1,n} \\\\\n",
    "\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{i,1} & \\dots & x_{i,j} &  \\dots & x_{i,n} \\\\\n",
    "\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{m,1} & \\dots & x_{m,j} & \\dots & x_{m,n} \\\\\n",
    "\\end{bmatrix}\n",
    "&\n",
    "=\n",
    "&\n",
    "(\\hat{\\textbf{t}}_i^T) \\rightarrow\n",
    "&\n",
    "\\begin{bmatrix} \n",
    "\\begin{bmatrix} \\, \\\\ \\, \\\\ \\textbf{u}_1 \\\\ \\, \\\\ \\,\\end{bmatrix} \n",
    "\\dots\n",
    "\\begin{bmatrix} \\, \\\\ \\, \\\\ \\textbf{u}_l \\\\ \\, \\\\ \\, \\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "&\n",
    "\\cdot\n",
    "&\n",
    "\\begin{bmatrix} \n",
    "s_1 & \\dots & 0 \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "0 & \\dots & s_l \\\\\n",
    "\\end{bmatrix}\n",
    "&\n",
    "\\cdot\n",
    "&\n",
    "\\begin{bmatrix} \n",
    "\\begin{bmatrix} & & \\textbf{v}_1 & & \\end{bmatrix} \\\\\n",
    "\\vdots \\\\\n",
    "\\begin{bmatrix} & & \\textbf{v}_l & & \\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "Components of this decomposition are sorted in order of data variance, explained by them; selecting first $l$ components we can obtain dense representation of each word-vector. You can calculate part of variance explained:\n",
    "\n",
    "$$\n",
    "R^2 = \\frac{\\sum_{i=0}^{l} s_i^2}{\\sum_{j=0}^{n} s_j^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. SVD is a linear transformation; therefore, it can not capture non-linear relations between words. \n",
    "2. Not so efficient as state-of-the-art methods (neural networks for example)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
