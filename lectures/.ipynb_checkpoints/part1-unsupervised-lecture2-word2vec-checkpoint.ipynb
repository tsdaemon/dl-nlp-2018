{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Usupervised learning\n",
    "\n",
    "© Anatolii Stehnii, 2018\n",
    "\n",
    "## Lecture 2: Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of deep learning is **representation learning**. How representation learning can be used to find dense word vectors?\n",
    "\n",
    "---\n",
    "*Returning to the previous lecture: \"Linguistic items with similar distributions have similar meanings\".*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning not always used to predict something; **representation of data** which is learned without supervision, also can be valuable (see autoencoders, neural language models etc).\n",
    "\n",
    "Word2vec exploit this approach by **training a model**, which takes each word and tries to predict all words from it's context.\n",
    "\n",
    "![Word2vec approach simple](Word2vec_words.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's denote:\n",
    "\n",
    "- $w_t$ – a word at step $t \\in 1..T$, center word;\n",
    "- $n$ – window size;\n",
    "- $w_{t+i}, i \\in [-n/2, 0) \\cap (0, n/2]$ – a word in a context of $w_t$; \n",
    "- $\\textbf{P}(w_{t+i}|w_t), $ – conditional probability of word $w_{t+i}$ given $w_t$;\n",
    "- $\\theta$ – model parameters (neural network weights).\n",
    "\n",
    "Then likelihood for neural network will be: \n",
    "\n",
    "$$\n",
    "\\textbf{L}(\\theta) = \\prod_{t=1}^T\\prod_{-n/2 \\le i \\le n/2, i \\ne 0} \\textbf{P}(w_{t+i}|w_t; \\theta)\n",
    "$$\n",
    "\n",
    "Objective function of average negative log likelihood:\n",
    "\n",
    "$$\n",
    "\\textbf{J}(\\theta) = -\\frac{1}{T}\\textbf{L}(\\theta) = -\\frac{1}{T}\\sum_{t=1}^T\\sum_{-n/2 \\le i \\le n/2, i \\ne 0} log \\textbf{P}(w_{t+i}|w_t; \\theta)\n",
    "$$\n",
    "\n",
    "$\\textbf{P}(w_{t+i}|w_t; \\theta)$ is a conditional probability, approximated by a neural network. Minimizing $\\textbf{J}(\\theta)$ with respect $\\theta$ we will get neural model, which predicts context of words, and model parameters $\\theta$ will actually be dense representations of words. But what is a function for $\\textbf{P}(w_{t+i}|w_t; \\theta)$?\n",
    "\n",
    "Let's define two matrices $\\textbf{W}_{in}$ and $\\textbf{W}_{out}$ of size $len(V) \\times m$, where $V$ is our vocabulary and $m$ is a desireable number of dimensions for word embeddings. This matrices will be all our parameters, so $\\theta = {\\textbf{W}_{in}, \\textbf{W}_{out}}$. Each word will have a correspoding row vector in both $\\textbf{W}_{in}$ and $\\textbf{W}_{out}$. Let's denote a center word as $c$ and a context word as $o$, and corresponding word vectors as $v_c$ and $u_o$.\n",
    "\n",
    "$$ \n",
    "\\textbf{P}(o|c; \\textbf{W}_{in}, \\textbf{W}_{out}) = \n",
    "\\frac{exp(v_c^T u_o)}{\\sum_{w \\in V} exp(v_c^T u_w)}\n",
    "$$\n",
    "\n",
    "Dot product in numerator $v_c^T u_o$ denotes similarity of words $c$ and $o$. Denominator and exponent normalizes it by similarities of $c$ and all other words in a vocabulary (softmax). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this formulas it could be pretty incomprehensive how it should work. Let's revise implementation of this formula and implement a forward propagation of a neural network.\n",
    "\n",
    "![Word2vec implementation](Word2vec_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center word one-hot encoding: [ 0.  0.  1.  0.]\n",
      "Context words one-hot encodings: \n",
      "[[ 1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "corpus = 'he was old man'\n",
    "vocab = {\n",
    "    'he': 0,\n",
    "    'was': 1,\n",
    "    'old': 2,\n",
    "    'man': 3\n",
    "\n",
    "}\n",
    "vocab_len = len(vocab)\n",
    "\n",
    "center_word = 2\n",
    "center_word_encoded = np.zeros((vocab_len, ))\n",
    "center_word_encoded[center_word] = 1\n",
    "\n",
    "# this a skip-gram implementation\n",
    "context_words = [0, 1, 3]\n",
    "context_words_encoded = np.zeros((len(context_words), vocab_len))\n",
    "context_words_encoded[np.arange(len(context_words)), context_words] = 1\n",
    "print('Center word one-hot encoding: {}\\nContext words one-hot encodings: \\n{}'.format(center_word_encoded, context_words_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W in matrix:\n",
      "[[ 0.24186189  0.89357085  0.26148547  0.23270299  0.49560695]\n",
      " [ 0.01482146  0.68605003  0.74224183  0.91068227  0.35435903]\n",
      " [ 0.8115948   0.18869666  0.463782    0.31674507  0.27754381]\n",
      " [ 0.2423285   0.35453277  0.30785807  0.14330915  0.49802627]]\n",
      "W out matrix:\n",
      "[[ 0.44731698  0.59629897  0.89405164  0.12812246  0.93850568]\n",
      " [ 0.10365351  0.11948719  0.87945264  0.7286056   0.0193697 ]\n",
      " [ 0.92330252  0.35593004  0.03198809  0.14778626  0.61177179]\n",
      " [ 0.79678261  0.79271956  0.85932509  0.38838388  0.11762184]]\n"
     ]
    }
   ],
   "source": [
    "# number of dimensions for word vectors\n",
    "n_dim = 5\n",
    "W_in = np.random.rand(vocab_len, n_dim)\n",
    "W_out = np.random.rand(vocab_len, n_dim)\n",
    "print('W in matrix:\\n{}\\nW out matrix:\\n{}'.format(W_in, W_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center word vector v: [ 0.8115948   0.18869666  0.463782    0.31674507  0.27754381]\n"
     ]
    }
   ],
   "source": [
    "v_c = np.dot(center_word_encoded, W_in)\n",
    "print('Center word vector v: {}'.format(v_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities:\n",
      "[ 1.19126342  0.75070395  1.04794988  1.35045156]\n"
     ]
    }
   ],
   "source": [
    "U_w = np.dot(v_c, W_out.T)\n",
    "print('Similarities:\\n{}'.format(U_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities:\n",
      "[ 0.27153864  0.17478296  0.23528344  0.31839495]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "conditional_probabilities = softmax(U_w)\n",
    "print('Probabilities:\\n{}'.format(conditional_probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities of context:\n",
      "[ 0.27153864  0.17478296  0.31839495]\n"
     ]
    }
   ],
   "source": [
    "probabilities_of_context_words = np.dot(conditional_probabilities, context_words_encoded.T)\n",
    "print('Probabilities of context:\\n{}'.format(probabilities_of_context_words)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative log likelihood: 4.192323783664577\n"
     ]
    }
   ],
   "source": [
    "negative_log_likelihood = -np.sum(np.log(probabilities_of_context_words))\n",
    "print('Negative log likelihood: {}'.format(negative_log_likelihood)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After minimization of loss function we will have two word-vector matrices: $\\textbf{W}_{in}$ and $\\textbf{W}_{out}$. We can use any of them or average value as word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
